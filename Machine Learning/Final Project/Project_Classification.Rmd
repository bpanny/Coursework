---
title: "INFSCI 2595 Fall 2022 Final Project Classification"
author: "Benjamin Panny"
date: "Submission time: December 13, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include = TRUE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Project Overview (from Project Guidelines PDF)

Surface coating helps materials people interact with last longer.

Coating is created by combining constituent materials together.

Constituents are combined in a manufacturing process following a specific set of operating conditions.

Coating is applied to a specimen and the speciment is subjected to an accelerated life test.

An accelerated life test includes conditions such as extreme temperatures and humidity for extended periods of time. These tests simulate years of typical material use. The outcome of these tests is the amount of corroded surface.

The aim of experimenting with surface coatings is to minimize the amount of corroded surface after the accelerated life test. Experiments can be augmented by training machine learning models that find the optimal constituents and process settings to minimize corrosion.

The goals of this project are as follows:

1. Build machine learning models and use the best model to identify input values that minimize the output. This involves training models to predict the fraction of corroded surface per test as a function of the inputs.
1. Understand which inputs are most important at causing the fraction of corroded surface to be less than 0.33. This involves identifying the best binary classifier.
1. Train models that use a mixture of provided inputs and derived features to identify if the derived features are as important as subject matter experts believe they are.
1. Understand if the machine used to manufacture coatings influences the test response.

# Data Description (from Project Guidelines PDF)

The inputs consist of three groups of variables:

* Chemistry variables: `x1, x2, x3, x4`
  * These are fractions between 0 and 1. Providing the proportion of a coating material constituted by the constituent.
  * A "balance" constituent, `x5`, is also present in the coating material. The fraction associated with the balance is `x5 = 1 - (x1 + x2 + x3 + x4)`. This means a subset of the first four inputs must be selected to include in the model when `x5` is included because including all of them will cause one of their coefficients to be undefined because of singularities. That is, through x5, each can be written as having an exact linear relationship with the sum of all the others. This means the effect of one input variable can't be estimated when holding all the other input variables constant because the change in the one is a change in the others by definition.
* Manufacturing process variables: `v1, v2, v3, v4, v5`
  * These represent how chemical constituents are combined to create a coating.
* Machine used to manufacture the coating: `m`
  * A categorical variable.

Derived features proposed by subject matter experts:
* The "w" ratio: `w = x2 / (x3 + x4)`
* The "z" ratio: `z = (x1 + x2) / (x4 + x5)`
* The product of v1 and v2: `t = v1 * v2`

The response variable (`output`) is the fraction of the specimen surface that corroded after the test completed.
* Regression models will be trained to predict the logit-transformed response: `y`
* Classification models will be trained to predict the threshold-transformed response: `outcome`

Rows/observations correspond to a test result and its inputs.

## Load Packages

```{r, load_packages}
library(tidyverse)
library(caret)
library(splines)
```

## Read Data, Derive Features, and Transform the Response

```{r, read_data}
df <- readr::read_csv('fall2022_finalproject.csv', col_names = TRUE)
```

```{r, derive_features_and_transform_response}
df <- df %>% 
  mutate(x5 = 1 - (x1 + x2 + x3 + x4),
         w = x2 / (x3 + x4),
         z = (x1 + x2) / (x5 + x4),
         t = v1 * v2,
         y = boot::logit(output),
         outcome = ifelse(output < 0.33, 'event', 'non_event'),
         outcome = factor(outcome, levels = c("event", "non_event")),
         outcome_y = ifelse(outcome == 'event', 1, 0))

df_nnet_reg <- df %>%
  select(-outcome, -outcome_y, -output)
```

Before exploring the data, convert the data to long format for certain visualizations.

```{r, long_format}
lf <- df %>% 
  tibble::rowid_to_column() %>% 
  pivot_longer(c(x1, x2, x3, x4, v1, v2, v3, v4, v5, x5, w, z, t))
```

Define some functions I'll use later to evaluate models


```{r, define_functions}
extract_metrics <- function(mod_object, mod_name)
{
  broom::glance(mod_object) %>% 
    mutate(model_name = mod_name)
}

extract_sig_coef <- function(mod_object)
{
  # Extract the p-values
  p_values <- summary(mod_object)$coefficients[, 4]
  
  # Find the indices of the coefficients that have p-values less than .05
  significant_indices <- which(p_values < 0.05)
  
  # Extract significant coefficients
  significant_coefs <- summary(mod_object)$coefficients[c(significant_indices),]
  significant_coefs
}
```

# Classification

## Non-Bayesian Generalized Linear Models (GLMs)

### Fitting Non-Bayesian GLMs

This code fits non-Bayesian Generalized Linear Models. Which are linear models that generalize the linear predictor to a target variable through a link function. In this case, the link function is the logistic function. All models fit in this section follow the same model formulas specified in the Regression section, except for the fact that the likelihood function is a binomial distribution instead of a Gaussian distribution. Similarly, the models do not predict the logit-transformed output, y, nor do they predict the raw output (the corrosion fraction), but instead the models predict the probability the corrosion fraction is less than .33.

```{r, base_features_nb_glm}
# all linear additive base features
glm_1 <- glm(outcome_y ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5, data = df, family = "binomial")

# interaction of the categorical input with all continuous inputs
glm_2 <- glm(outcome_y ~ m*(x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5), data = df, family = "binomial")

# all pair-wise interactions of the continuous inputs
glm_3 <- glm(outcome_y ~ (x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5)^2, data = df, family = "binomial")
```

```{r, expanded_features_nb_glm}
# all linear additive expanded feature set
glm_4 <- glm(outcome_y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z, data = df, family = "binomial")

# interaction of the categorical input with all continuous features
glm_5 <- glm(outcome_y ~ m*(x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z), data = df, family = "binomial")

# all pair-wise interactions of the continuous features
glm_6 <- glm(outcome_y ~ (x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z)^2, data = df, family = "binomial")
```

The following models try to fit more complex linear basis functions using splines. lm_9 will use splines on the entire expanded feature set (excluding the machines as any interactions or additive effects with the machines seem very minor). lm_7 will use splines on the features with apparent non-linear relationships with the output (x1 and z (excluding x5 for the sake of avoiding singularities and features with high correlations we know of a priori)), while using the rest as additive effects. lm_8 will do the same, but will include the continuous interactions that were significant in the initial 6 models.

```{r, glm_basis_models}
ndf <- 7

glm_7 <- glm(outcome_y ~ t + v1 + v2 + v3 + v4 + v5 + w + ns(x1, ndf) + x2 + x3 + x4 + ns(z, ndf), data = df, family = "binomial")

glm_8 <- glm(outcome_y ~ t + v1 + v2 + v3 + v4 + v5 + w + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:v1 + x1:v4 + x1:v5 + x2:x3 + x2:v5 + x3:v1 + x3:v4 + x4:v2 + ns(x1, ndf) + ns(z, ndf), data = df, family = "binomial")

glm_9 <- glm(outcome_y ~ ns(t, ndf) + ns(v1, ndf) + ns(v2, ndf) + ns(v3, ndf) + ns(v4, ndf) + ns(v5, ndf) + ns(w, ndf) + ns(x1, ndf) + ns(x2, ndf) + ns(x3, ndf) + ns(x4, ndf) + ns(z, ndf), data = df, family = "binomial")
```

### Evaluating Performance of Non-Bayesian GLMs

```{r, evaluate_nbglm}
glm_mle_results <- purrr::map2_dfr(list(glm_1, glm_2, glm_3, glm_4,
                                        glm_5, glm_6, glm_7, glm_8,
                                        glm_9),
                                   1:9,
                                   extract_metrics)
glm_mle_results %>% 
  select(model_name, AIC, BIC) %>% 
  pivot_longer(c(AIC, BIC)) %>% 
  ggplot(mapping = aes(x = model_name, y = value)) +
  geom_point(size = 5) +
  facet_wrap(~name, scales = 'free_y') +
  theme_bw()

```

The AIC/BIC metrics are consistent in terms of the top three models, but AIC selects glm_9 as the best model while BIC selects glm_7 as the best model (due to greater complexity cost outweighing the error reduction of glm_9 under the BIC). Since BIC penalizes complexity more and I am mindful of overfitting, I will choose glm_7 as the best performing model on the training dataset, though an evaluation of their true performance depends on test data and approximating performance on test data with cross-validation and resampling.

### Visualizing Coefficient Summaries

```{r, viz_coef_summaries_glm}
coefplot::coefplot(glm_7) + theme_bw() +
  theme(legend.position = 'none')
coefplot::coefplot(glm_8) + theme_bw() +
  theme(legend.position = 'none')
coefplot::coefplot(glm_9) + theme_bw() +
  theme(legend.position = 'none')
```

The coefficients for splines of the z ratio are consistently significant across glm_7, glm_8, and glm_9. The splines for x1 are significant in glm_7 and glm_9, but not as much in glm_8. glm_8 has massive magnitude and uncertainty regarding the x2:x3 interaction coefficient. glm_9 also has large magnitude and uncertainty in several of its coefficients. glm_7 has the smallest coefficient magnitudes and uncertainties, though they are still quite large.

### Identifying Significant Features

The below code prints the significant features for the top three GLMs.

```{r, nbglm_coef_summaries}
glm_7 %>% extract_sig_coef()
glm_8 %>% extract_sig_coef()
glm_9 %>% extract_sig_coef()
```

The splines for x1 and z are significant and important in the top three models. Similarly, the "w" ratio seems important to these classification models as well, since it was significant as a main additive effect and as a feature to derive splines from.

## Bayesian Generalized Linear Models

### Fitting Bayesian GLMs

```{r, load_rstanarm_glm}
library(rstanarm)
library(rstan)
```

This section fits the two best non-Bayesian generalized linear models according to AIC/BIC from the previous section with Bayesian techniques. A Student t prior with 7 degrees of freedom and a scale of 2.5 will be used, which is "a reasonable default prior when coefficients should be close to zero but have some chance of being large" according to [this Stan vignette](https://mc-stan.org/rstanarm/articles/binomial.html)

```{r, mc_options}
options(mc.cores = parallel::detectCores())
```


```{r, fit_bglm_7, cache = TRUE}
t_prior <- student_t(df = 7, location = 0, scale = 2.5) #
bglm_7 <- stan_glm(outcome_y ~ t + v1 + v2 + v3 + v4 + v5 + w + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:v1 + x1:v4 + x1:v5 + x2:x3 + x2:v5 + x3:v1 + x3:v4 + x4:v2 + ns(x1, ndf) + ns(z, ndf), data = df,
                   family = binomial(link = "logit"),
                   prior = t_prior, prior_intercept = t_prior,
                   seed = 123456)
```

```{r, fit_bglm_9, cache = TRUE}
bglm_9 <- stan_glm(outcome_y ~ ns(t, ndf) + ns(v1, ndf) + ns(v2, ndf) + ns(v3, ndf) + ns(v4, ndf) + ns(v5, ndf) + ns(w, ndf) + ns(x1, ndf) + ns(x2, ndf) + ns(x3, ndf) + ns(x4, ndf) + ns(z, ndf), data = df,
                  family = binomial(link = "logit"),
                  prior = t_prior, prior_intercept = t_prior,
                  seed = 123456)
```


### Evaluating Performance of Bayesian Linear Models

```{r, eval_bglms, cache = TRUE}
waic_bglm_7 <- waic(bglm_7)
waic_bglm_9 <- waic(bglm_9)
loo_bglm_7 <- loo(bglm_7)
loo_bglm_9 <- loo(bglm_9)
bglm_comp <- loo_compare(loo_bglm_7, loo_bglm_9)
waic_bglm_weights_results <- loo_model_weights(list(`7` = loo_bglm_7, `9` = loo_bglm_9))
waic_bglm_weights_results
```

According to model weighting derived from the LOOCV metric, bglm_9 is a better Bayesian generalized linear model than bglm_7.

```{r, viz_posterior_summary_statistics_bglm_9}
bglm_9_summary <- bglm_9$stan_summary
bglm_9_summary <- bglm_9_summary %>% as_tibble() %>% mutate(name = rownames(bglm_9_summary))
bglm_9_summary %>% filter(name != "log-posterior") %>%
  ggplot(mapping = aes(x = mean, y = as.factor(name))) +
  geom_point(color = 'blue') +
  geom_errorbar(mapping = aes(xmin = `2.5%`, xmax = `97.5%`), color = 'blue')
coefplot::coefplot(glm_9)
```

In comparison to the non-bayesian glm_9, bglm_9 has largely the same splines that are statistically significant. However, the prior seems to be limiting the magnitude of the coefficients and keeping them closer to zero, while still allowing them some breadth.

## Generalized Linear Model Predictions

```{r, extract_sig_coef_glm}
glm_7 %>% extract_sig_coef()
glm_9 %>% extract_sig_coef()
```

The splines of z and x1 are significant in both models. The "w" ratio also seems to be an important input for both models, therefore I will predict the mean event probability trends along values of z and w, while keeping the other inputs constant at their observed medians. An important assumption here is that the trends visualized based on z and w, when they are not derived from true values of x1, x2, x3, x4, and x5, are representative of the trend based on z and w when they are derived from true values.

```{r, tidy_predict_glm_function_link}
tidy_predict_glm_link <- function(mod, xnew)
{
  pred_df <- predict(mod, xnew, type = "link", se.fit = TRUE) %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    mutate(ci_lwr = mod$family$linkinv(fit + qt(0.025, df = mod$df.residual) * se.fit),
           event_prob = mod$family$linkinv(fit),
           ci_upr = mod$family$linkinv(fit + qt(0.975, df = mod$df.residual) * se.fit))
  
  xnew %>% bind_cols(pred_df)
}
```

```{r, make_viz_grid_class}
viz_grid_class <- expand.grid(t = median(df$t),
                        v1 = median(df$v1),
                        v2 = median(df$v2),
                        v3 = median(df$v3),
                        v4 = median(df$v4),
                        v5 = median(df$v5),
                        w = seq(min(df$w), max(df$w), length.out = 9),
                        x1 = median(df$x1),
                        x2 = median(df$x2),
                        x3 = median(df$x3),
                        x4 = median(df$x4),
                        z = seq(min(df$z), max(df$z), length.out = 101),
                        KEEP.OUT.ATTRS = FALSE,
                        stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()

```

```{r, glm_predictions, cache = TRUE}
pred_glm_7 <- tidy_predict_glm_link(glm_7, viz_grid_class)
pred_glm_9 <- tidy_predict_glm_link(glm_9, viz_grid_class)
```


```{r, plot_glm_predictive_mean_trends}
pred_glm_7 %>% 
  ggplot(mapping = aes(x = z)) +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'grey') +
  geom_line(mapping = aes(y = event_prob),
            color = 'black')  +
  facet_wrap(~w, labeller = "label_both") +
  theme_bw()

pred_glm_9 %>% 
  ggplot(mapping = aes(x = z)) +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'grey') +
  geom_line(mapping = aes(y = event_prob),
            color = 'black')  +
  facet_wrap(~w, labeller = "label_both") +
  theme_bw()

```

The predictive trends between the glm_7 and glm_9 appear largely consistent with some subtle differences. Overall, the "z" ratio in each shows an inverted U pattern for the mean event probability as the "z" ratio value increases. The main differences appear to be that bglm_9, for several values of w, has an almost box-like shape when predicting the event, whereas glm_7 often has less sharp curvature. Additionally, sometimes glm_9 seems to have larger uncertainty around its trend curves than glm_7 and other times glm_7 seems to have larger uncertainty than glm_9. This is subtle, however, in that the uncertainty in the predictive trend of glm_7 tends to be more consistent across values of z and w, whereas uncertainty appears to sharply increase and decrease across different values of z in glm_9's predictive trend.

## Training GLMs with Cross-validation, Resampling, and Regularization

The following code sets the number of folds and samples for training models with cross-validation and resampling as well as performance metrics

```{r, set_glm_ctrl_metric}
set.seed(123456)
ctrl_acc <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
ctrl_roc <- trainControl(method = 'repeatedcv', number = 10, repeats = 3,
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE,
                         savePredictions = TRUE)
metric_acc <- 'Accuracy'
metric_roc <- 'ROC'
```

The following code trains models of the additive effects of the base features (glm_1), the additive effects of the expanded feature set (glm_4), and the two spline models (glm_7 and glm_9). Each model formulation will be trained on accuracy and ROC metrics separately.

```{r, caret_glm_train_acc, cache = TRUE}
glm_1_acc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5, 
                    data = df,
                    method = "glm",
                    preProcess = c("center", "scale"),
                    metric = metric_acc,
                    trControl = ctrl_acc)

glm_4_acc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                    data = df,
                    method = "glm",
                    metric = metric_acc,
                    preProcess = c("center", "scale"),
                    trControl = ctrl_acc)

glm_7_acc <- train(outcome ~ t + v1 + v2 + v3 + v4 + v5 + w + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:v1 + x1:v4 + x1:v5 + x2:x3 + x2:v5 + x3:v1 + x3:v4 + x4:v2 + ns(x1, 7) + ns(z, 7),
                    data = df,
                    method = "glm",
                    metric = metric_acc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_acc)

glm_9_acc <- train(outcome ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "glm",
                    metric = metric_acc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_acc)
```

```{r, caret_glm_train_roc, cache = TRUE}
glm_1_roc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5, 
                    data = df,
                    method = "glm",
                    preProcess = c("center", "scale"),
                    metric = metric_roc,
                    trControl = ctrl_roc)

glm_4_roc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                    data = df,
                    method = "glm",
                    metric = metric_roc,
                    preProcess = c("center", "scale"),
                    trControl = ctrl_roc)

glm_7_roc <- train(outcome ~ t + v1 + v2 + v3 + v4 + v5 + w + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:v1 + x1:v4 + x1:v5 + x2:x3 + x2:v5 + x3:v1 + x3:v4 + x4:v2 + ns(x1, 7) + ns(z, 7),
                    data = df,
                    method = "glm",
                    metric = metric_roc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_roc)

glm_9_roc <- train(outcome ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "glm",
                    metric = metric_roc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_roc)
```

The below code trains regularized regression models with the elastic net penalty.

```{r, class_enet_acc_roc, cache = TRUE}
glm_int_enet_acc <- train(outcome ~ m*((x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z)^2),
                    data = df,
                    method = "glmnet",
                    metric = metric_acc,
                    preProcess = c("center", "scale"),
                    trControl = ctrl_acc)

glm_9_enet_acc <- train(outcome ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "glmnet",
                    metric = metric_acc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_acc)

glm_int_enet_roc <- train(outcome ~ m*((x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z)^2),
                    data = df,
                    method = "glmnet",
                    metric = metric_roc,
                    preProcess = c("center", "scale"),
                    trControl = ctrl_roc)

glm_9_enet_roc <- train(outcome ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "glmnet",
                    metric = metric_roc,
    preProcess = c("center", "scale"),
                    trControl = ctrl_roc)

plot(glm_int_enet_acc)
glm_int_enet_acc$bestTune
plot(glm_int_enet_roc)
glm_int_enet_roc$bestTune
plot(glm_9_enet_acc)
glm_9_enet_acc$bestTune
plot(glm_9_enet_roc)
glm_9_enet_roc$bestTune
```

For glm_int_enet, using the default tuning grid of `glmnet`, the lasso penalty was applied regardless of whether the performance metric was accuracy or ROC. However, the optimal model according to the ROC had a larger penalty factor compared to the optimal model according to Accuracy.

For glm_9_enet, using the default tuning grid, the lasso penalty was applied and the same lambda penalty factor was optimal regardless of using accuracy or ROC as performance metric.

## Training Classification Models with Non-Linear Methods

The below code trains a neural net on a custom tuning grid.

```{r, class_nnet, cache = TRUE}
nnet_grid <- expand.grid(size = c(5,9,13,17),
                         decay = exp(seq(-6, 0, length.out = 11)))

nnet_base_acc <- caret::train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                          data = df,
                          method = 'nnet',
                          metric = metric_acc,
                          trControl = ctrl_acc,
                          preProcess = c('center', 'scale'),
                          trace = FALSE,
                          tuneGrid = nnet_grid)

nnet_exp_acc <- caret::train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                          data = df,
                          method = 'nnet',
                          metric = metric_acc,
                          trControl = ctrl_acc,
                          preProcess = c('center', 'scale'),
                          trace = FALSE,
                          tuneGrid = nnet_grid)

nnet_base_roc <- caret::train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                          data = df,
                          method = 'nnet',
                          metric = metric_roc,
                          trControl = ctrl_roc,
                          preProcess = c('center', 'scale'),
                          trace = FALSE,
                          tuneGrid = nnet_grid)

nnet_exp_roc <- caret::train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                          data = df,
                          method = 'nnet',
                          metric = metric_roc,
                          trControl = ctrl_roc,
                          preProcess = c('center', 'scale'),
                          trace = FALSE,
                          tuneGrid = nnet_grid)

plot(nnet_base_acc)
nnet_base_acc$bestTune
plot(nnet_base_roc)
nnet_base_roc$bestTune
plot(nnet_exp_acc)
nnet_exp_acc$bestTune
plot(nnet_exp_roc)
nnet_exp_roc$bestTune
```

The optimal values for nnet trained on base features is size = 13, decay = .549 when accuracy is the performance metric and size = 17, decay = .549 when ROC is the performance metric.

The optimal values for nnet trained on the expanded feature set is size = 9, decay = .549 when accuracy is the performance metric and size = 9, decay = .549 when ROC is the performance metric.

The below code trains a random forest model

```{r, regression_rf, cache = TRUE}
rf_base_acc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                     data = df,
                     method = 'rf',
                     metric = metric_acc,
                     trControl = ctrl_acc,
                     importance = TRUE)

rf_exp_acc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                    data = df,
                    method = 'rf',
                    metric = metric_acc,
                    trControl = ctrl_acc,
                    importance = TRUE)

rf_base_roc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                     data = df,
                     method = 'rf',
                     metric = metric_roc,
                     trControl = ctrl_roc,
                     importance = TRUE)

rf_exp_roc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                    data = df,
                    method = 'rf',
                    metric = metric_roc,
                    trControl = ctrl_roc,
                    importance = TRUE)

rf_base_acc
rf_base_roc
rf_exp_acc
rf_exp_roc
```

When ROC is the performance metric used, optimal values of mtry are in the middle of the tuning grid (5, and 7, respectively, for the base and expanded feature set). When accuracy is the performance metric used, the optimal values of mtry are the high end of the tuning grid equal to the number of input features (9 and 12, respectively, for the base and expanded feature sets).

The below code trains a gradient-boosted tree via XGBoost.

```{r, regression_xgb, cache = TRUE}
xgb_base_acc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                      data = df,
                      method = 'xgbTree',
                      trControl = ctrl_acc,
                      metric = metric_acc,
                      verbosity = 0)

xgb_exp_acc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'xgbTree',
                     trControl = ctrl_acc,
                     metric = metric_acc,
                     verbosity = 0)

xgb_base_roc <- train(outcome ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                      data = df,
                      method = 'xgbTree',
                      trControl = ctrl_roc,
                      metric = metric_roc,
                      verbosity = 0)

xgb_exp_roc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'xgbTree',
                     trControl = ctrl_roc,
                     metric = metric_roc,
                     verbosity = 0)

plot(xgb_base_acc)
xgb_base_acc$bestTune
plot(xgb_exp_acc)
xgb_exp_acc$bestTune
plot(xgb_base_roc)
xgb_base_roc$bestTune
plot(xgb_exp_roc)
xgb_exp_roc$bestTune
```

## Training Partial Least Squares and Multivariate Adaptive Regression Spline Classification Models

```{r, class_pls, cache = TRUE}
pls_exp_acc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'pls',
                     trControl = ctrl_acc,
                     metric = metric_acc,
                     preProcess = c('center', 'scale'),
                     tuneLength = 20)

pls_exp_roc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'pls',
                     trControl = ctrl_roc,
                     metric = metric_roc,
                     preProcess = c('center', 'scale'),
                     tuneLength = 20)

plot(pls_exp_acc)
pls_exp_acc
plot(pls_exp_roc)
pls_exp_roc
```

The optimal number of components for the PLS model is 12 when it is selected by maximizing accuracy and it is 13 when it is selected by maximizing the area under the ROC curve.

```{r, class_mars, cache = TRUE, warning=FALSE}
mars_exp_acc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'earth',
                     trControl = ctrl_acc,
                     metric = metric_acc,
                     preProcess = c('center', 'scale'),
                     tuneGrid = expand.grid(nprune = seq(2, 50, by = 4),
                         degree = seq(1, 3, by = 1)))

mars_exp_roc <- train(outcome ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'earth',
                     trControl = ctrl_roc,
                     metric = metric_roc,
                     preProcess = c('center', 'scale'),
                     tuneGrid = expand.grid(nprune = seq(2, 50, by = 4),
                         degree = seq(1, 3, by = 1)))

plot(mars_exp_acc)
mars_exp_acc$bestTune
plot(mars_exp_roc)
mars_exp_roc$bestTune
```

The best model when accuracy is used to select it uses tuning parameters ntune = 22 and degree = 1. The best model when area under the ROC curve is used to select it uses tuning parameters nprune = 26 and degree = 2.

## Classification Model Comparison

The below code compares the linear models trained through cross-validation, resampling, and the elastic net penalty, as well as the neural networks and tree-based models.

```{r, caret_class_comparisons}
caret_acc_compare <- resamples(
  list(base_add = glm_1_acc,
       expanded_add = glm_4_acc,
       add_splines_7_df = glm_7_acc,
       splines_7_df = glm_9_acc,
       expanded_int_enet = glm_int_enet_acc,
       splines_7_df_enet = glm_9_enet_acc,
       nnet_base = nnet_base_acc,
       nnet_expanded = nnet_exp_acc,
       rf_base = rf_base_acc,
       rf_expanded = rf_exp_acc,
       xgb_base = xgb_base_acc,
       xgb_expanded = xgb_exp_acc,
       pls_expanded = pls_exp_acc,
       mars_expanded = mars_exp_acc))

caret_roc_compare <- resamples(
  list(base_add = glm_1_roc,
       expanded_add = glm_4_roc,
       add_splines_7_df = glm_7_roc,
       splines_7_df = glm_9_roc,
       expanded_int_enet = glm_int_enet_roc,
       splines_7_df_enet = glm_9_enet_roc,
       nnet_base = nnet_base_roc,
       nnet_expanded = nnet_exp_roc,
       rf_base = rf_base_roc,
       rf_expanded = rf_exp_roc,
       xgb_base = xgb_base_roc,
       xgb_expanded = xgb_exp_roc,
       pls_expanded = pls_exp_roc,
       mars_expanded = mars_exp_roc))

dotplot(caret_acc_compare)
dotplot(caret_roc_compare)

```

The best model, by Accuracy and Area under the ROC curve metrics, is the gradient-boosted tree trained on the expanded feature set.

```{r, save_xgb_regression_models}
xgb_base_acc %>% readr::write_rds("xgb_base_acc.rds")
xgb_exp_acc %>% readr::write_rds("xgb_exp_acc.rds")
xgb_base_roc %>% readr::write_rds("xgb_base_roc.rds")
xgb_exp_roc %>% readr::write_rds("xgb_exp_roc.rds")
```
