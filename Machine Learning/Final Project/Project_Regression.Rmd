---
title: "INFSCI 2595 Fall 2022 Final Project Regression"
author: "Benjamin Panny"
date: "Submission time: December 13, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Project Overview (from Project Guidelines PDF)

Surface coating helps materials people interact with last longer.

Coating is created by combining constituent materials together.

Constituents are combined in a manufacturing process following a specific set of operating conditions.

Coating is applied to a specimen and the speciment is subjected to an accelerated life test.

An accelerated life test includes conditions such as extreme temperatures and humidity for extended periods of time. These tests simulate years of typical material use. The outcome of these tests is the amount of corroded surface.

The aim of experimenting with surface coatings is to minimize the amount of corroded surface after the accelerated life test. Experiments can be augmented by training machine learning models that find the optimal constituents and process settings to minimize corrosion.

The goals of this project are as follows:

1. Build machine learning models and use the best model to identify input values that minimize the output. This involves training models to predict the fraction of corroded surface per test as a function of the inputs.
1. Understand which inputs are most important at causing the fraction of corroded surface to be less than 0.33. This involves identifying the best binary classifier.
1. Train models that use a mixture of provided inputs and derived features to identify if the derived features are as important as subject matter experts believe they are.
1. Understand if the machine used to manufacture coatings influences the test response.

# Data Description (from Project Guidelines PDF)

The inputs consist of three groups of variables:

* Chemistry variables: `x1, x2, x3, x4`
  * These are fractions between 0 and 1. Providing the proportion of a coating material constituted by the constituent.
  * A "balance" constituent, `x5`, is also present in the coating material. The fraction associated with the balance is `x5 = 1 - (x1 + x2 + x3 + x4)`. This means a subset of the first four inputs must be selected to include in the model when `x5` is included because including all of them will cause one of their coefficients to be undefined because of singularities. That is, through x5, each can be written as having an exact linear relationship with the sum of all the others. This means the effect of one input variable can't be estimated when holding all the other input variables constant because the change in the one is a change in the others by definition.
* Manufacturing process variables: `v1, v2, v3, v4, v5`
  * These represent how chemical constituents are combined to create a coating.
* Machine used to manufacture the coating: `m`
  * A categorical variable.

Derived features proposed by subject matter experts:
* The "w" ratio: `w = x2 / (x3 + x4)`
* The "z" ratio: `z = (x1 + x2) / (x4 + x5)`
* The product of v1 and v2: `t = v1 * v2`

The response variable (`output`) is the fraction of the specimen surface that corroded after the test completed.
* Regression models will be trained to predict the logit-transformed response: `y`
* Classification models will be trained to predict the threshold-transformed response: `outcome`

Rows/observations correspond to a test result and its inputs.

## Load Packages

```{r, load_packages}
library(tidyverse)
library(caret)
library(splines)
```

## Read Data, Derive Features, and Transform the Response

```{r, read_data}
df <- readr::read_csv('fall2022_finalproject.csv', col_names = TRUE)
```

```{r, derive_features_and_transform_response}
df <- df %>% 
  mutate(x5 = 1 - (x1 + x2 + x3 + x4),
         w = x2 / (x3 + x4),
         z = (x1 + x2) / (x5 + x4),
         t = v1 * v2,
         y = boot::logit(output),
         outcome = ifelse(output < 0.33, 'event', 'non_event'),
         outcome = factor(outcome, levels = c("event", "non_event")),
         outcome_y = ifelse(outcome == 'event', 1, 0))

df_nnet_reg <- df %>%
  select(-outcome, -outcome_y, -output)
```

Before exploring the data, convert the data to long format for certain visualizations.

```{r, long_format}
lf <- df %>% 
  tibble::rowid_to_column() %>% 
  pivot_longer(c(x1, x2, x3, x4, v1, v2, v3, v4, v5, x5, w, z, t))
```

Define some functions I'll use later to evaluate models


```{r, define_functions}
extract_metrics <- function(mod_object, mod_name)
{
  broom::glance(mod_object) %>% 
    mutate(model_name = mod_name)
}

extract_sig_coef <- function(mod_object)
{
  # Extract the p-values
  p_values <- summary(mod_object)$coefficients[, 4]
  
  # Find the indices of the coefficients that have p-values less than .05
  significant_indices <- which(p_values < 0.05)
  
  # Extract significant coefficients
  significant_coefs <- summary(mod_object)$coefficients[c(significant_indices),]
  significant_coefs
}
```

# Regression

## Non-Bayesian Linear Models

### Fitting Non-Bayesian Linear Models

Start by fitting non-bayesian linear models 

```{r, base_features_nb_linear_models}
# all linear additive base features
lm_1 <- lm(y ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5, data = df)

# interaction of the categorical input with all continous inputs
lm_2 <- lm(y ~ m*(x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5), data = df)

# all pair-wise interactions of the continuous inputs
lm_3 <- lm(y ~ (x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5)^2, data = df)
```

```{r, expanded_features_nb_linear_models}
# all linear additive expanded feature set
lm_4 <- lm(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z, data = df)

# interaction of the categorical input with all continuous features
lm_5 <- lm(y ~ m*(x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z), data = df)

# all pair-wise interactions of the continuous features
lm_6 <- lm(y ~ (x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z)^2, data = df)
```

Before defining other linear basis functions. I will check the significant features of the initial six models to see if there are any continuous input interactions that may be worth visualizing.

```{r, check_sig_feats_lms_with_interactions}
lm_2 %>% extract_sig_coef()
lm_3 %>% extract_sig_coef()
lm_5 %>% extract_sig_coef()
lm_6 %>% extract_sig_coef()
```

It appears that there are some interactions between the categorical and continuous inputs, which are visualized in the "Visualize Conditional Distributions" section in the EDA document. A few interactions between continuous inputs seem significant, so I will visualize those that are present in both lm_3 and lm_6, which are x1:x3, x1:v4, and x3:v1

The following models try to fit more complex linear basis functions using splines. lm_9 will use splines on the entire expanded feature set (excluding the machines as any interactions or additive effects with the machines seem very minor). lm_7 will use splines on the features with apparent non-linear relationships with the output (x1 and z (excluding x5 for the sake of avoiding singularities and features with high correlations we know of a priori)), while using the rest as additive effects. lm_8 will do the same, but will include the continuous interactions that were significant in the initial 6 models.

```{r, linear_basis_models}
ndf <- 7

lm_7 <- lm(y ~ t + v1 + v2 + v3 + v4 + v5 + w + ns(x1, ndf) + x2 + x3 + x4 + ns(z, ndf), data = df)

lm_8 <- lm(y ~ t + v1 + v2 + v3 + v4 + v5 + w + x2 + x3 + x4 + x1:x2 + x1:x3 + x1:v1 + x1:v4 + x1:v5 + x2:x3 + x2:v5 + x3:v1 + x3:v4 + x4:v2 + ns(x1, ndf) + ns(z, ndf), data = df)

lm_9 <- lm(y ~ ns(t, ndf) + ns(v1, ndf) + ns(v2, ndf) + ns(v3, ndf) + ns(v4, ndf) + ns(v5, ndf) + ns(w, ndf) + ns(x1, ndf) + ns(x2, ndf) + ns(x3, ndf) + ns(x4, ndf) + ns(z, ndf), data = df)
```

### Evaluating Performance of Non-Bayesian Linear Models

I will use BIC and AIC to identify the top three non-Bayesian linear models I fit.

```{r, evaluate_nblm}
lm_mle_results <- purrr::map2_dfr(list(lm_1, lm_2, lm_3, lm_4,
                                        lm_5, lm_6, lm_7, lm_8,
                                        lm_9),
                                   1:9,
                                   extract_metrics)
lm_mle_results %>% 
  select(model_name, AIC, BIC) %>% 
  pivot_longer(c(AIC, BIC)) %>% 
  ggplot(mapping = aes(x = model_name, y = value)) +
  geom_point(size = 5) +
  facet_wrap(~name, scales = 'free_y') +
  theme_bw()

```

The best three models according to AIC/BIC appear to be the models that include splines (lm_7, lm_8, lm_9)

### Visualize Coefficient Summaries

```{r, viz_coef_summaries_lm}
coefplot::coefplot(lm_7) + theme_bw() +
  theme(legend.position = 'none')
coefplot::coefplot(lm_8) + theme_bw() +
  theme(legend.position = 'none')
coefplot::coefplot(lm_9) + theme_bw() +
  theme(legend.position = 'none')
```

The spline features in each of the models appear to be consistently significant across lm_7, lm_8, and lm_9. lm_8 stands out in that some of the interaction coefficients are large in magnitude and highly uncertain. Additionally, it seems that interactions terms or additive terms are more likely to be significant if they involve a constituent input compared to manufacturing process inputs. The inputs that seem important are x1, z, and perhaps other constituent inputs depending on the model formulation context.

### Identify Significant Features

Let's evaluate the important inputs from each model

```{r, nblm_coef_summaries}
lm_7 %>% extract_sig_coef()
lm_8 %>% extract_sig_coef()
lm_9 %>% extract_sig_coef()
```

Across the models, spline-derived features appear important, otherwise it seems that interaction terms including constituent inputs are more likely to be significant than those including manufacturing process variables, while manufacturing process variables are only significant in their additive effects when the model doesn't account for interactions. It is difficult to determine the significance of these observations, since lm_8 and lm_7 seem to perform similarly.

## Bayesian Linear Models

### Fitting Bayesian Linear Models

```{r, load_rstanarm}
library(rstanarm)
library(rstan)
```

This section fits the two best non-Bayesian linear models according to AIC/BIC from the previous section with Bayesian techniques. lm_9 was clearly the best, but second best seemed to be a tie between lm_8 and lm_7. I will choose lm_7 as my second Bayesian linear model because it achieves virtually the same performance as lm_8 with less features, in addition to the fact that the interaction coefficients for lm_8 are very large and uncertain.

```{r, mcmc_options}
options(mc.cores = parallel::detectCores())
```


```{r, fit_blm, cache = TRUE}
blm_7 <- stan_lm(y ~ t + v1 + v2 + v3 + v4 + v5 + w + ns(x1, 7) + x2 + x3 + x4 + ns(z, 7), data = df,
                 prior = R2(location = 0.5),
                 seed = 123456) 

```

```{r, fit_blm_9, cache = TRUE}
blm_9 <- stan_lm(y ~ ns(t, ndf) + ns(v1, ndf) + ns(v2, ndf) + ns(v3, ndf) + ns(v4, ndf) + ns(v5, ndf) + ns(w, ndf) + ns(x1, ndf) + ns(x2, ndf) + ns(x3, ndf) + ns(x4, ndf) + ns(z, ndf), data = df,
                 prior = R2(location = 0.5),
                 seed = 123456)
```

### Evaluating Performance of Bayesian Linear Models

```{r, eval_blms, cache = TRUE}
waic_blm_7 <- waic(blm_7)
waic_blm_9 <- waic(blm_9)
loo_blm_7 <- loo(blm_7)
loo_blm_9 <- loo(blm_9)
blm_comp <- loo_compare(loo_blm_7, loo_blm_9)
waic_model_weights_results <- loo_model_weights(list(`7` = loo_blm_7, `9` = loo_blm_9))
waic_model_weights_results
```

According to model weighting derived from the LOOCV metric. blm_9 is a better Bayesian linear model than blm_7.

Let's visualize the posterior summary statistics for blm_9

```{r, viz_posterior_summary_statistics_blm_9}
blm_9_summary <- blm_9$stan_summary
blm_9_summary <- blm_9_summary %>% as_tibble() %>% mutate(name = rownames(blm_9_summary))
blm_9_summary %>% filter(name != "log-posterior") %>%
  ggplot(mapping = aes(x = mean, y = as.factor(name))) +
  geom_point(color = 'blue') +
  geom_errorbar(mapping = aes(xmin = `2.5%`, xmax = `97.5%`), color = 'blue') +
  labs(title = "Coefficients from blm_9")
coefplot::coefplot(lm_9, title = "Coefficients from lm_9")
```

The MLE estimates and posterior mean estimates from the Bayesian model appear the same.

The noise, $\sigma$, is the residual error of the model's mean trend. 

```{r, viz_sigma_post}
purrr::map2_dfr(list(blm_7, blm_9),
                as.character(c(7, 9)),
                function(mod, mod_name){as.data.frame(mod) %>% tibble::as_tibble() %>% 
                    select(sigma) %>% 
                    mutate(model_name = mod_name)}) %>% 
  ggplot(mapping = aes(x = sigma)) +
  geom_freqpoly(bins = 55,
                 mapping = aes(color = model_name),
                 size = 1.1) +
  geom_vline(xintercept = summary(lm_9)$sigma, label = "MLE on sigma") +
  annotate("text", x= summary(lm_9)$sigma -.01, y = 500, label="MLE on Sigma from lm_9", angle=90) +
  ggthemes::scale_color_colorblind("Model") +
  theme_bw() 
```

The posterior uncertainty in the residual error is centered around 0.8. The MLE on sigma from lm_9 is reasonably within the posterior uncertainty on sigma for the Bayesian model formulation. The posterior seems quite certain about sigma. That is, the probable values for sigma are mostly contained within 0.05 units of the approximate center of 0.8.


## Linear Model Predictions
  
This function organizes predictions from the non-Bayesian linear models to visualize trends in the logit-transformed response, y, with respect to the inputs.

```{r, tidy_predict_function}
tidy_predict <- function(mod, xnew)
{
  pred_df <- predict(mod, xnew, interval = "confidence") %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    dplyr::select(pred = fit, ci_lwr = lwr, ci_upr = upr) %>% 
    bind_cols(predict(mod, xnew, interval = 'prediction') %>% 
                as.data.frame() %>% tibble::as_tibble() %>% 
                dplyr::select(pred_lwr = lwr, pred_upr = upr))
  
  xnew %>% bind_cols(pred_df)
}
```

This chunk defines a grid of inputs to make predictions on with models lm_7 and lm_9. Since the spline features from x1 and z account for a large proportion of variance in the response, I will define the inputs to make predictions mostly along z and x1. An important assumption here is that the trends visualized based on z, when z is not derived from true values of x1, x2, x3, x4, and x5, are representative of the trend based on z when it is derived from true values.

```{r, make_viz_grid}
viz_grid <- expand.grid(t = median(df$t),
                        v1 = median(df$v1),
                        v2 = median(df$v2),
                        v3 = median(df$v3),
                        v4 = median(df$v4),
                        v5 = median(df$v5),
                        w = median(df$w),
                        x1 = seq(min(df$x1), max(df$x1), length.out = 9),
                        x2 = median(df$x2),
                        x3 = median(df$x3),
                        x4 = median(df$x4),
                        z = seq(min(df$z), max(df$z), length.out = 101),
                        KEEP.OUT.ATTRS = FALSE,
                        stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()

```

```{r, lm_predictions, cache = TRUE}

lm_pred_7 <- tidy_predict(lm_7, viz_grid)

lm_pred_9 <- tidy_predict(lm_9, viz_grid)

```

```{r, viz_lm_predictions}
lm_pred_7 %>% 
  ggplot(mapping = aes(x = z)) +
  geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr),
              fill = 'orange') +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'grey') +
  geom_line(mapping = aes(y = pred),
            color = 'black')  +
  facet_wrap(~x1, labeller = "label_both") +
  theme_bw()

lm_pred_9 %>% 
  ggplot(mapping = aes(x = z)) +
  geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr),
              fill = 'orange') +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'grey') +
  geom_line(mapping = aes(y = pred),
            color = 'black')  +
  facet_wrap(~x1, labeller = "label_both") +
  theme_bw()
```

The predictive trends are consistent between the two non-Bayesian linear models, lm_7 and lm_9, though the prediction intervals seem wider for lm_7 than  lm_9, while the confidence intervals for lm_9 is wider than that for lm_7.

## Training Linear Models with Cross-Validation, Resampling, and Regularization

The following code sets the number of folds and samples for training models with cross-validation and resampling

```{r, set_lm_ctrl_metric}
set.seed(123456)
my_ctrl_lm <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
my_metric_lm <- 'RMSE'
```

The following code trains models of the additive effects of the base features (lm_1), the additive effects of the expanded feature set (lm_4), and the two of the models including splines (lm_7 and lm_9)

```{r, caret_lm_train, cache = TRUE}
lm_1_caret <- train(as.formula(lm_1$model), 
                    data = df,
                    method = "lm",
                    metric = my_metric_lm,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)

lm_4_caret <- train(as.formula(lm_4$model),
                    data = df,
                    method = "lm",
                    metric = my_metric_lm,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)

lm_7_caret <- train(y ~ t + v1 + v2 + v3 + v4 + v5 + w + ns(x1, 7) + x2 + x3 + x4 + ns(z, 7),
                    data = df,
                    method = "lm",
                    metric = my_metric_lm,
    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)

lm_9_caret <- train(y ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "lm",
                    metric = my_metric_lm,
    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)
```


The below code trains regularized regression models with the elastic net penalty.

```{r, regression_enet, cache = TRUE}
lm_int_enet <- train(y ~ m*((x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z)^2),
                    data = df,
                    method = "glmnet",
                    metric = my_metric_lm,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)

lm_9_enet <- train(y ~ ns(t, 7) + ns(v1, 7) + ns(v2, 7) + ns(v3, 7) + 
    ns(v4, 7) + ns(v5, 7) + ns(w, 7) + ns(x1, 7) + 
    ns(x2, 7) + ns(x3, 7) + ns(x4, 7) + ns(z, 7),
                    data = df,
                    method = "glmnet",
                    metric = my_metric_lm,
    preProcess = c("center", "scale"),
                    trControl = my_ctrl_lm)

lm_int_enet
lm_9_enet
```

Tuning with the default grid, lm_int_enet appears to use the Lasso penalty, while lm_9_enet appears to a use a mix of the Lasso and Ridge penalty with a lower penalty factor than lm_int_enet. This may be due to high correlations in the spline features and the large number of features derived.

## Training Models with Non-Linear Methods

The below code trains a neural net on a custom tuning grid.

```{r, regression_nnet, cache = TRUE}
nnet_grid <- expand.grid(size = c(5,9,13,17),
                         decay = exp(seq(-6, 0, length.out = 11)))

nnet_base_reg <- caret::train(y ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                          data = df,
                          method = 'nnet',
                          metric = my_metric_lm,
                          trControl = my_ctrl_lm,
                          preProcess = c('center', 'scale'),
                          linout = TRUE,
                          trace = FALSE,
                          tuneGrid = nnet_grid)

nnet_exp_reg <- caret::train(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                          data = df,
                          method = 'nnet',
                          metric = my_metric_lm,
                          trControl = my_ctrl_lm,
                          preProcess = c('center', 'scale'),
                          linout = TRUE,
                          trace = FALSE,
                          tuneGrid = nnet_grid)

plot(nnet_base_reg)
nnet_base_reg$bestTune
plot(nnet_exp_reg)
nnet_exp_reg$bestTune
```

Given the custom tuning grid, for the base features, the optimal tuning parameters were size = 13 and decay = 0.02732372, while for the expanded feature set the optimal tuning parameters were size = 13 and decay = 0.002478752

The below code trains a random forest model

```{r, regression_rf, cache = TRUE}
rf_base_reg <- train(y ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                     data = df,
                     method = 'rf',
                     metric = my_metric_lm,
                     trControl = my_ctrl_lm,
                     importance = TRUE)

rf_exp_reg <- train(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                    data = df,
                    method = 'rf',
                    metric = my_metric_lm,
                    trControl = my_ctrl_lm,
                    importance = TRUE)

rf_base_reg
rf_exp_reg
```

Using the default tuning grid, the optimal number of variables selected per split for the two random forest models was the number of inputs to the model. That is, 13 for the RF of base features and 16 for the expanded set when assessing model performance by RMSE

The below code trains a gradient-boosted tree via XGBoost.

```{r, regression_xgb, cache = TRUE}
xgb_base_reg <- train(y ~ m + x1 + x2 + x3 + x4 + v1 + v2 + v3 + v4 + v5,
                      data = df,
                      method = 'xgbTree',
                      trControl = my_ctrl_lm,
                      metric = my_metric_lm,
                      verbosity = 0)

xgb_exp_reg <- train(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'xgbTree',
                     trControl = my_ctrl_lm,
                     metric = my_metric_lm,
                     verbosity = 0)

plot(xgb_base_reg)
xgb_base_reg$bestTune
plot(xgb_exp_reg)
xgb_exp_reg$bestTune
```

## Training Partial Least Squares Models and Multivariate Adaptive Regression Spline Models

```{r, regression_pls, cache = TRUE}
pls_exp_reg <- train(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'pls',
                     trControl = my_ctrl_lm,
                     metric = my_metric_lm,
                     preProcess = c('center', 'scale'),
                     tuneLength = 20)
pls_exp_reg
```

The optimal number of components for the PLS model is 14.

```{r, regression_mars, cache = TRUE}
mars_exp_reg <- train(y ~ m + x1 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + t + w + z,
                     data = df,
                     method = 'earth',
                     trControl = my_ctrl_lm,
                     metric = my_metric_lm,
                     preProcess = c('center', 'scale'),
                     tuneGrid = expand.grid(nprune = seq(2, 50, by = 4),
                         degree = seq(1, 3, by = 1)))
mars_exp_reg
```

The degree of interaction for the final MARS model is 3, with 50 terms, though the MARS models with 22 terms and 2 degrees of interaction are comparable.

## Regression Model Comparison

The below code compares the linear models trained through cross-validation, resampling, and the elastic net penalty, as well as the neural networks and tree-based models.

```{r, caret_comparisons}
caret_rmse_compare <- resamples(list(base_add = lm_1_caret,
                                    expanded_add = lm_4_caret,
                                    expanded_add_splines_7_df = lm_7_caret,
                                    splines_7_df = lm_9_caret,
                                    expanded_int_enet = lm_int_enet,
                                    splines_7_df_enet = lm_9_enet,
                                    nnet_base = nnet_base_reg,
                                    nnet_expanded = nnet_exp_reg,
                                    rf_base = rf_base_reg,
                                    rf_expanded = rf_exp_reg,
                                    xgb_base = xgb_base_reg,
                                    xgb_expanded = xgb_exp_reg,
                                    pls_expanded = pls_exp_reg,
                                    mars_expanded = mars_exp_reg))

dotplot(caret_rmse_compare)
```

The best model, by MAE, RMSE, and Rsquared metrics, is the gradient-boosted tree trained on the expanded feature set.

```{r, save_xgb_regression_models}
xgb_base_reg %>% readr::write_rds("xgb_base_reg.rds")
xgb_exp_reg %>% readr::write_rds("xgb_exp_reg.rds")
```

