{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def read_in_shakespeare():\n",
    "    \"\"\"Reads in the Shakespeare dataset and processes it into a list of tuples.\n",
    "       Also reads in the vocab and play name lists from files.\n",
    "\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "    Returns:\n",
    "      tuples: A list of tuples in the above format.\n",
    "      document_names: A list of the plays present in the corpus.\n",
    "      vocab: A list of all tokens in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    tuples = []\n",
    "\n",
    "    with open(\"shakespeare_plays.csv\") as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "\n",
    "            tuples.append((play_name, line_tokens))\n",
    "\n",
    "    with open(\"vocab.txt\") as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "\n",
    "    with open(\"play_names.txt\") as f:\n",
    "        document_names = [line.strip() for line in f]\n",
    "\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "    \"\"\"A convenience function to get a particular row vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      row_id: an integer row_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the row vector\n",
    "    \"\"\"\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    \"\"\"A convenience function to get a particular column vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      col_id: an integer col_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the column vector\n",
    "    \"\"\"\n",
    "    return matrix[:, col_id]\n",
    "\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    \"\"\"\n",
    "    # Check for 0 vectors\n",
    "    if not np.any(vector1) or not np.any(vector2):\n",
    "        sim = 0\n",
    "\n",
    "    else:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(vector1, vector2)\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    \"\"\"Returns a numpy array containing the term document matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      document_names: A list of the document names\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "      td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    m = len(vocab)\n",
    "    n = len(document_names)\n",
    "    term_doc = np.zeros(m * n).reshape(m, n)\n",
    "    term_doc = pd.DataFrame(term_doc, columns=document_names, index=vocab)\n",
    "    for line in line_tuples:\n",
    "        document_name = line[0]\n",
    "        words = line[1]\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "              term_doc[document_name][word] += 1\n",
    "\n",
    "    # smooth and 'scale'\n",
    "    term_doc = np.log10(term_doc + 1)\n",
    "    term_doc = term_doc.to_numpy()\n",
    "\n",
    "    return term_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I.I Vector Spaces and Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n"
     ]
    }
   ],
   "source": [
    "tuples, document_names, vocab = read_in_shakespeare()\n",
    "\n",
    "print(\"Computing term document matrix...\")\n",
    "td_matrix = create_term_document_matrix(tuples, document_names, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "    \"\"\"Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "    See section 6.5 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_document_matrix: Numpy array where each column represents a document\n",
    "      and each row, the frequency of a word in that document.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_document_matrix, where\n",
    "      A_ij is weighted by the inverse document frequency of document h.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    tf_idf_matrix = term_document_matrix.copy()\n",
    "    for word in range(0, td_matrix.shape[0]): \n",
    "        tf = get_row_vector(term_document_matrix, word)\n",
    "        df = sum(tf > 0)\n",
    "        idf = np.log10(len(document_names) / df)\n",
    "        tf_idf_matrix[word,:] = tf * idf\n",
    "    \n",
    "    return tf_idf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tf-idf matrix...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Computing tf-idf matrix...\")\n",
    "tf_idf_matrix = create_tf_idf_matrix(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    \"\"\"Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let n = len(vocab).\n",
    "\n",
    "    Returns:\n",
    "      tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "          word j was found within context_window_size to the left or right of\n",
    "          word i in any sentence in the tuples.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    n = len(vocab)\n",
    "    tc_matrix = lil_matrix((n, n), dtype=int)\n",
    "    vocab_index, index = {}, 0 \n",
    "    for word in vocab:\n",
    "        vocab_index[word] = index\n",
    "        index += 1\n",
    "\n",
    "    # inverse_vocab_index = {index: word for word, index in vocab_index.items()}\n",
    "\n",
    "    def unnest_and_merge(nested_list):\n",
    "        merged_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                merged_list.extend(unnest_and_merge(item))\n",
    "            else:\n",
    "                merged_list.append(item)\n",
    "        return merged_list\n",
    "    \n",
    "    # make document dictionary\n",
    "    for line in tuples:\n",
    "        doc_name = line[0]\n",
    "        words = line[1]\n",
    "        if doc_name in doc_dict:\n",
    "            doc_dict[doc_name].extend(words)  \n",
    "        else:\n",
    "            doc_dict[doc_name] = words \n",
    "\n",
    "    doc_dict = {}\n",
    "\n",
    "    for doc in doc_dict.keys():\n",
    "        doc_dict[doc] = unnest_and_merge(doc_dict[doc])\n",
    "        doc_length = len(doc_dict[doc])\n",
    "        context_bag = []\n",
    "        for word in set(doc_dict[doc]):\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            word_indexes = [i for i, doc_words in enumerate(doc_dict[doc]) if doc_words == word]\n",
    "            for idx in word_indexes:\n",
    "                # Check for if word is close to start or end of document, given context_window_size\n",
    "                if  context_window_size <= idx <= (doc_length - 3):\n",
    "                    context_bag = doc_dict[doc][(idx - context_window_size):(idx + context_window_size + 1)]\n",
    "                elif context_window_size > idx: \n",
    "                    l_context = context_window_size - (context_window_size - idx)\n",
    "                    context_bag = doc_dict[doc][(idx - l_context):(idx + context_window_size + 1)]\n",
    "                elif idx > (doc_length - 1 - context_window_size):\n",
    "                    r_context = doc_length - idx\n",
    "                    context_bag = doc_dict[doc][(idx - context_window_size):(idx + r_context + 1)]\n",
    "                for context_word in context_bag:\n",
    "                    if context_word not in vocab:\n",
    "                        continue\n",
    "                    tc_matrix[vocab_index[word], vocab_index[context_word]] += 1\n",
    "            \n",
    "            tc_matrix[vocab_index[word], vocab_index[word]] += -len(word_indexes)\n",
    "            \n",
    "    return(tc_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term context matrix...\n",
      "3718\n",
      "3717\n",
      "3922\n",
      "3440\n",
      "3377\n",
      "3163\n",
      "3769\n",
      "2435\n",
      "3865\n",
      "4048\n",
      "4537\n",
      "4392\n",
      "3504\n",
      "3430\n",
      "2782\n",
      "3996\n",
      "3625\n",
      "3197\n",
      "3219\n",
      "3155\n",
      "3174\n",
      "2904\n",
      "2895\n",
      "3647\n",
      "3138\n",
      "3511\n",
      "3885\n",
      "3535\n",
      "3140\n",
      "3073\n",
      "3176\n",
      "3295\n",
      "4110\n",
      "3018\n",
      "2633\n",
      "3707\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing term context matrix...\")\n",
    "tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ppmi_matrix(term_context_matrix):\n",
    "    \"\"\"Given the term context matrix, output a PPMI weighted version.\n",
    "\n",
    "    See section 6.6 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_context_matrix: Numpy array where each column represents a context word\n",
    "      and each row, the frequency of a word that occurs with that context word.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_context_matrix, where\n",
    "      A_ij is weighted by PPMI.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    total_word_count = np.sum(term_context_matrix)\n",
    "    word_counts = np.sum(term_context_matrix, axis=1) / total_word_count\n",
    "    context_counts = np.sum(term_context_matrix, axis=0) / total_word_count\n",
    "\n",
    "    ppmi_matrix = lil_matrix((term_context_matrix.shape[0], term_context_matrix.shape[1]))\n",
    "\n",
    "    for word in range(term_context_matrix.shape[0]):\n",
    "        nonzero_contexts = np.nonzero(get_row_vector(term_context_matrix, word))\n",
    "        for context in nonzero_contexts[0]:\n",
    "            word_context_p = term_context_matrix[word, context] / total_word_count\n",
    "            outer_word_context_p = word_counts[word] * context_counts[context]\n",
    "            if word_context_p == 0.0:\n",
    "                ppmi_matrix[word, context] = 0.0\n",
    "            else:\n",
    "                ppmi_matrix[word, context] = max(np.log2(word_context_p / outer_word_context_p), 0.0)\n",
    "                \n",
    "    return(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PPMI matrix...\")\n",
    "ppmi_matrix = create_ppmi_matrix(tc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words(target_word_index, matrix):\n",
    "    \"\"\"Ranks the similarity of all of the words to the target word using compute_cosine_similarity.\n",
    "\n",
    "    Inputs:\n",
    "      target_word_index: The index of the word we want to compare all others against.\n",
    "      matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "\n",
    "    Returns:\n",
    "      A length-n list of integer word indices, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "      A length-n list of similarity scores, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    similarity = []\n",
    "    if isinstance(matrix, lil_matrix):\n",
    "      target_vector = matrix[target_word_index].toarray()[0]\n",
    "      for i in range(matrix.shape[0]): \n",
    "          if i == target_word_index:\n",
    "            continue\n",
    "          \n",
    "          # Convert only the row of the matrix that is currently evaluated with i to a numpy array\n",
    "          tmp_comparison_vector = matrix[i].toarray()[0]\n",
    "          \n",
    "          similarity.append(compute_cosine_similarity(target_vector, tmp_comparison_vector))\n",
    "    else:\n",
    "      target_vector = matrix[target_word_index, :]\n",
    "      for i in range(matrix.shape[0]): \n",
    "        if i == target_word_index:\n",
    "          continue\n",
    "      \n",
    "        similarity.append(compute_cosine_similarity(target_vector, matrix[i]))\n",
    "\n",
    "    word_and_sim = list(zip([i for i in range(0, matrix.shape[0]) if i != target_word_index], similarity))\n",
    "\n",
    "    sorted_word_and_sim = sorted(word_and_sim, key=lambda x: x[1], reverse = True)\n",
    "\n",
    "    sorted_words = [sorted[0] for sorted in sorted_word_and_sim]\n",
    "    sorted_sims = [sorted[1] for sorted in sorted_word_and_sim]\n",
    "\n",
    "    return sorted_words, sorted_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on term-document frequency matrix are:\n",
      "1: procures; 0.9611235625180706\n",
      "2: benedicite; 0.9611235625180706\n",
      "3: ghostly; 0.9380710616293597\n",
      "4: capulets; 0.8748623439660793\n",
      "5: mercutio; 0.8748623439660793\n",
      "6: capulet; 0.8748623439660792\n",
      "7: pump; 0.8748623439660792\n",
      "8: laura; 0.8748623439660792\n",
      "9: pitcher; 0.8748623439660792\n",
      "10: behoveful; 0.8748623439660792\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on term-context frequency matrix are:\n",
      "1: am; 1\n",
      "2: warwick; 0.7863009277715228\n",
      "3: maria; 0.7813181188793948\n",
      "4: helena; 0.7788038614889767\n",
      "5: cloten; 0.7687813263801225\n",
      "6: clown; 0.7673992667724928\n",
      "7: touchstone; 0.7671713498011156\n",
      "8: gloucester; 0.7663929420831365\n",
      "9: jaquenetta; 0.7625294119579562\n",
      "10: diomedes; 0.7592329642096558\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on tf-idf matrix are:\n",
      "1: procures; 0.9611235625180706\n",
      "2: benedicite; 0.9611235625180706\n",
      "3: ghostly; 0.9380710616293598\n",
      "4: pump; 0.8748623439660792\n",
      "5: mercutio; 0.8748623439660792\n",
      "6: households; 0.8748623439660792\n",
      "7: blubbering; 0.8748623439660792\n",
      "8: stinted; 0.8748623439660792\n",
      "9: duellist; 0.8748623439660792\n",
      "10: switch; 0.8748623439660792\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on PPMI matrix are:\n",
      "1: throe; 0.1784176591434079\n",
      "2: hist; 0.1760453035422327\n",
      "3: ursula; 0.15724404397747238\n",
      "4: silvia; 0.15320819864340685\n",
      "5: benumbed; 0.15156266689461984\n",
      "6: pe; 0.14629131220181457\n",
      "7: unrecuring; 0.14621967997890617\n",
      "8: opposeless; 0.146182789702897\n",
      "9: banditti; 0.1460769309986898\n",
      "10: inseparable; 0.14349439839154998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# random_idx = random.randint(0, len(document_names) - 1)\n",
    "\n",
    "word = \"juliet\"\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "\n",
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-document frequency matrix are:'\n",
    "    % (word)\n",
    ")\n",
    "ranks, scores = rank_words(vocab_to_index[word], td_matrix)\n",
    "for idx in range(0,10):\n",
    "    word_id = ranks[idx]\n",
    "    print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "    % (word)\n",
    ")\n",
    "ranks, scores = rank_words(vocab_to_index[word], tc_matrix)\n",
    "for idx in range(0,10):\n",
    "    word_id = ranks[idx]\n",
    "    print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "\n",
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on tf-idf matrix are:'\n",
    "    % (word)\n",
    ")\n",
    "ranks, scores = rank_words(vocab_to_index[word], tf_idf_matrix)\n",
    "for idx in range(0,10):\n",
    "    word_id = ranks[idx]\n",
    "    print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on PPMI matrix are:'\n",
    "    % (word)\n",
    ")\n",
    "ranks, scores = rank_words(vocab_to_index[word], ppmi_matrix)\n",
    "for idx in range(0,10):\n",
    "    word_id = ranks[idx]\n",
    "    print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
