{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will build unigram, bigram, and trigram character language models (both unsmoothed and smoothed versions) for three languages, score a test document with each, and determine the language it is written in based on perplexity. You will also use your English language models to generate texts. You will critically examine all results. The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement basic smoothing and interpolation.\n",
    "- Use the perplexity of a language model to perform language identification.\n",
    "- Use a language model to probabilistically generate texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this project is available here: hw3_data.zip. It consists of:\n",
    "\n",
    "- training.en - English training data\n",
    "- training.es - Spanish training data\n",
    "- training.de - German training data\n",
    "- test - test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filenames = ['hw3_data/training.en.txt', 'hw3_data/training.de.txt', 'hw3_data/training.es.txt', 'hw3_data/test.txt']\n",
    "with open(filenames[0], 'r') as f:\n",
    "    en = f.read().lower()\n",
    "with open(filenames[1], 'r') as f:\n",
    "    de = f.read().lower()\n",
    "with open(filenames[2], 'r') as f:\n",
    "    es = f.read().lower()\n",
    "with open(filenames[3], 'r') as f:\n",
    "    test = f.read().lower()\n",
    "\n",
    "# en = pd.read_csv('hw3_data/training.en.txt', sep=' ', header = None)\n",
    "# es = pd.read_csv()\n",
    "# de = pd.read_csv()\n",
    "# test = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train character n-gram language models\n",
    "To complete the assignment, you will need to write a program (from scratch) that:\n",
    "\n",
    "- builds the models: reads in training data, collects counts for all character 1, 2, and 3-grams, estimates probabilities, and writes out the unigram, bigram, and trigram models (ngram probabilities) into files. Build separate models for each language.\n",
    "- adjusts the counts: rebuilds the bigram and trigram language models using linear interpolation with lambdas equally weighted\n",
    "\n",
    "You may make any additional assumptions and design decisions, but state them in your report (see below). For example, some design choices that could be made are how you want to handle uppercase and lowercase letters or how you want to handle digits. The choice made is up to you, we only require that you detail these decisions in your report and consider any implications of them in your results. There is no wrong choice here, and these decisions are typically made by NLP researchers when pre-processing data.\n",
    "\n",
    "You may write your program in any TA-approved programming language (Python, Java, C/C++).\n",
    "\n",
    "For this assignment you must implement the model generation from scratch, but you are allowed to use any resources or packages that help you manage your project, i.e. Github or any file i/o packages. If you have questions about this please ask.\n",
    "\n",
    "Extra credit (3 points): Also implement add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_en = []\n",
    "for line in en.split('\\n'):\n",
    "    tmp_en.append('<s>')\n",
    "    tmp_en.extend(list(line))\n",
    "    tmp_en.append('</s>')\n",
    "tmp_es = []\n",
    "for line in es.split('\\n'):\n",
    "    tmp_en.append('<s>')\n",
    "    tmp_es.extend(list(line))\n",
    "    tmp_en.append('</s>')\n",
    "tmp_de = []\n",
    "for line in de.split('\\n'):\n",
    "    tmp_en.append('<s>')\n",
    "    tmp_de.extend(list(line))\n",
    "    tmp_en.append('</s>')\n",
    "en = tmp_en\n",
    "es = tmp_es\n",
    "de = tmp_de\n",
    "del tmp_en, tmp_es, tmp_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_uni_counts = {}\n",
    "for char in en:\n",
    "    if char in en_uni_counts:\n",
    "        en_uni_counts[char] += 1\n",
    "    else:\n",
    "        en_uni_counts[char] = 1\n",
    "\n",
    "es_uni_counts = {}\n",
    "for char in es:\n",
    "    if char in es_uni_counts:\n",
    "        es_uni_counts[char] += 1\n",
    "    else:\n",
    "        es_uni_counts[char] = 1\n",
    "\n",
    "de_uni_counts = {}\n",
    "for char in de:\n",
    "    if char in de_uni_counts:\n",
    "        de_uni_counts[char] += 1\n",
    "    else:\n",
    "        de_uni_counts[char] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "en_bigrams = [(en[i], en[i+1]) for i in range(len(en) - 1)]\n",
    "en_bi_counts = {}\n",
    "for bigram in en_bigrams:\n",
    "    if bigram in en_bi_counts:\n",
    "        en_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        en_bi_counts[bigram] = 1\n",
    "\n",
    "\n",
    "en_bi_matrix = np.zeros((len(en_uni_counts), len(en_uni_counts)))\n",
    "en_uni_index = {unigram: index for index, unigram in enumerate(en_uni_counts)}\n",
    "for bigram, count in en_bi_counts.items():\n",
    "    en_bi_matrix[en_uni_index[bigram[0]], en_uni_index[bigram[1]]] = count\n",
    "\n",
    "es_bigrams = [(es[i], es[i+1]) for i in range(len(es) - 1)]\n",
    "es_bi_counts = {}\n",
    "for bigram in es_bigrams:\n",
    "    if bigram in es_bi_counts:\n",
    "        es_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        es_bi_counts[bigram] = 1\n",
    "\n",
    "es_bi_matrix = np.zeros((len(es_uni_counts), len(es_uni_counts)))\n",
    "es_uni_index = {unigram: index for index, unigram in enumerate(es_uni_counts)}\n",
    "for bigram, count in es_bi_counts.items():\n",
    "    es_bi_matrix[es_uni_index[bigram[0]], es_uni_index[bigram[1]]] = count\n",
    "\n",
    "de_bigrams = [(de[i], de[i+1]) for i in range(len(de) - 1)]\n",
    "de_bi_counts = {}\n",
    "for bigram in de_bigrams:\n",
    "    if bigram in de_bi_counts:\n",
    "        de_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        de_bi_counts[bigram] = 1\n",
    "\n",
    "de_bi_matrix = np.zeros((len(de_uni_counts), len(de_uni_counts)))\n",
    "de_uni_index = {unigram: index for index, unigram in enumerate(de_uni_counts)}\n",
    "for bigram, count in de_bi_counts.items():\n",
    "    de_bi_matrix[de_uni_index[bigram[0]], de_uni_index[bigram[1]]] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_trigrams = [(en[i], en[i+1], en[i+2]) for i in range(len(en) - 2)]\n",
    "en_tri_counts = {}\n",
    "for trigram in en_trigrams:\n",
    "    if trigram in en_tri_counts:\n",
    "        en_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        en_tri_counts[trigram] = 1\n",
    "\n",
    "en_tri_matrix = np.zeros((len(en_bi_counts), len(en_uni_counts)))\n",
    "en_bi_index = {bigram: index for index, bigram in enumerate(en_bi_counts)}\n",
    "for trigram, count in en_tri_counts.items():\n",
    "    en_tri_matrix[en_bi_index[trigram[0:2]], en_uni_index[trigram[2]]] = count\n",
    "\n",
    "es_trigrams = [(es[i], es[i+1], es[i+2]) for i in range(len(es) - 2)]\n",
    "es_tri_counts = {}\n",
    "for trigram in es_trigrams:\n",
    "    if trigram in es_tri_counts:\n",
    "        es_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        es_tri_counts[trigram] = 1\n",
    "\n",
    "es_tri_matrix = np.zeros((len(es_bi_counts), len(es_uni_counts)))\n",
    "es_bi_index = {bigram: index for index, bigram in enumerate(es_bi_counts)}\n",
    "for trigram, count in es_tri_counts.items():\n",
    "    es_tri_matrix[es_bi_index[trigram[0:2]], es_uni_index[trigram[2]]] = count\n",
    "\n",
    "de_trigrams = [(de[i], de[i+1], de[i+2]) for i in range(len(de) - 2)]\n",
    "de_tri_counts = {}\n",
    "for trigram in de_trigrams:\n",
    "    if trigram in de_tri_counts:\n",
    "        de_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        de_tri_counts[trigram] = 1\n",
    "\n",
    "de_tri_matrix = np.zeros((len(de_bi_counts), len(de_uni_counts)))\n",
    "de_bi_index = {bigram: index for index, bigram in enumerate(de_bi_counts)}\n",
    "for trigram, count in de_tri_counts.items():\n",
    "    de_tri_matrix[de_bi_index[trigram[0:2]], de_uni_index[trigram[2]]] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"writes out the unigram, bigram, and trigram models (ngram probabilities) into files. Build separate models for each language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert ngrams into probabilities and write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_total = sum(en_uni_counts.values())\n",
    "es_total = sum(es_uni_counts.values())\n",
    "de_total = sum(de_uni_counts.values())\n",
    "\n",
    "en_uni_probs = {key: value / en_total for key, value in en_uni_counts.items()}\n",
    "es_uni_probs = {key: value / es_total for key, value in es_uni_counts.items()}\n",
    "de_uni_probs = {key: value / de_total for key, value in de_uni_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(en_uni_probs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 62) (64, 64) (63, 63)\n"
     ]
    }
   ],
   "source": [
    "print(en_bi_matrix.shape, es_bi_matrix.shape, de_bi_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_bi_sums = en_bi_matrix.sum(axis = 1)[:, np.newaxis] # sum cols (words) of each row (context)\n",
    "en_bi_sums[en_bi_sums == 0] = 1 # no div by 0\n",
    "en_bi_matrix_norm = en_bi_matrix / en_bi_sums\n",
    "\n",
    "es_bi_sums = es_bi_matrix.sum(axis = 1)[:, np.newaxis] # sum over cols (words) of each row (context)\n",
    "es_bi_sums[es_bi_sums == 0] = 1 # no div by 0\n",
    "es_bi_matrix_norm = es_bi_matrix / es_bi_sums\n",
    "\n",
    "de_bi_sums = de_bi_matrix.sum(axis = 1)[:, np.newaxis] # sum cols (words) of each row (context)\n",
    "de_bi_sums[de_bi_sums == 0] = 1 # no div by 0\n",
    "de_bi_matrix_norm = de_bi_matrix / de_bi_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(en_bi_matrix_norm.sum(axis=1), es_bi_matrix_norm.sum(axis=1), de_bi_matrix_norm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(858, 62) (858, 64) (961, 63)\n"
     ]
    }
   ],
   "source": [
    "print(en_tri_matrix.shape, es_tri_matrix.shape, de_tri_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tri_sums = en_tri_matrix.sum(axis = 1)[:, np.newaxis] \n",
    "en_tri_sums[en_tri_sums == 0] = 1 # no div by 0\n",
    "en_tri_matrix_norm = en_tri_matrix / en_tri_sums\n",
    "\n",
    "es_tri_sums = es_tri_matrix.sum(axis = 1)[:, np.newaxis] \n",
    "es_tri_sums[es_tri_sums == 0] = 1 # no div by 0\n",
    "es_tri_matrix_norm = es_tri_matrix / es_tri_sums\n",
    "\n",
    "de_tri_sums = de_tri_matrix.sum(axis = 1)[:, np.newaxis] \n",
    "de_tri_sums[de_tri_sums == 0] = 1 # no div by 0\n",
    "de_tri_matrix_norm = de_tri_matrix / de_tri_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.]\n"
     ]
    }
   ],
   "source": [
    "print(en_tri_matrix_norm.sum(axis=1), es_tri_matrix_norm.sum(axis=1), de_tri_matrix_norm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adjusts the counts: rebuilds the bigram and trigram language models using linear interpolation with lambdas equally weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_en_uni_index = {value: key for key, value in en_uni_index.items()}\n",
    "reverse_es_uni_index = {value: key for key, value in es_uni_index.items()}\n",
    "reverse_de_uni_index = {value: key for key, value in de_uni_index.items()}\n",
    "\n",
    "en_bi_matrix_ip = np.zeros(en_bi_matrix.shape)\n",
    "for i in range(en_bi_matrix.shape[0]):\n",
    "    for j in range(en_bi_matrix.shape[1]):\n",
    "        en_bi_matrix_ip[i,j] = 0.5 * en_uni_counts[reverse_en_uni_index[j]] + 0.5 * en_bi_matrix[i,j]\n",
    "\n",
    "es_bi_matrix_ip = np.zeros(es_bi_matrix.shape)\n",
    "for i in range(es_bi_matrix.shape[0]):\n",
    "    for j in range(es_bi_matrix.shape[1]):\n",
    "        es_bi_matrix_ip[i,j] = 0.5 * es_uni_counts[reverse_es_uni_index[j]] + 0.5 * es_bi_matrix[i,j]\n",
    "\n",
    "de_bi_matrix_ip = np.zeros(de_bi_matrix.shape)\n",
    "for i in range(de_bi_matrix.shape[0]):\n",
    "    for j in range(de_bi_matrix.shape[1]):\n",
    "        de_bi_matrix_ip[i,j] = 0.5 * de_uni_counts[reverse_de_uni_index[j]] + 0.5 * de_bi_matrix[i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_bi_sums_ip = en_bi_matrix_ip.sum(axis = 1)[:, np.newaxis]\n",
    "en_bi_sums_ip[en_bi_sums_ip == 0] = 1 # no div by 0\n",
    "en_bi_matrix_ip_norm = en_bi_matrix_ip / en_bi_sums_ip\n",
    "\n",
    "es_bi_sums_ip = es_bi_matrix_ip.sum(axis = 1)[:, np.newaxis]\n",
    "es_bi_sums_ip[es_bi_sums_ip == 0] = 1 # no div by 0\n",
    "es_bi_matrix_ip_norm = es_bi_matrix_ip / es_bi_sums_ip\n",
    "\n",
    "de_bi_sums_ip = de_bi_matrix_ip.sum(axis = 1)[:, np.newaxis]\n",
    "de_bi_sums_ip[de_bi_sums_ip == 0] = 1 # no div by 0\n",
    "de_bi_matrix_ip_norm = de_bi_matrix_ip / de_bi_sums_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(en_bi_matrix_ip_norm.sum(axis=1))\n",
    "print(es_bi_matrix_ip_norm.sum(axis=1))\n",
    "print(de_bi_matrix_ip_norm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_en_bi_index = {value: key for key, value in en_bi_index.items()}\n",
    "reverse_es_bi_index = {value: key for key, value in es_bi_index.items()}\n",
    "reverse_de_bi_index = {value: key for key, value in de_bi_index.items()}\n",
    "\n",
    "en_tri_matrix_ip = np.zeros(en_tri_matrix.shape)\n",
    "for i in range(en_tri_matrix.shape[0]):\n",
    "    # give me the second character in the rownames of tri\n",
    "    bi_row = en_uni_index[reverse_en_bi_index[i][0]]\n",
    "    for j in range(en_tri_matrix.shape[1]):\n",
    "        uni_col = reverse_en_uni_index[j]\n",
    "        bi_col = en_uni_index[reverse_en_bi_index[i][1]]\n",
    "        en_tri_matrix_ip[i,j] = 1/3 * en_uni_counts[uni_col] + 1/3 * en_bi_matrix[bi_row,bi_col] + 1/3 * en_tri_matrix[i,j]\n",
    "\n",
    "es_tri_matrix_ip = np.zeros(es_tri_matrix.shape)\n",
    "for i in range(es_tri_matrix.shape[0]):\n",
    "    bi_row = es_uni_index[reverse_es_bi_index[i][0]]\n",
    "    for j in range(es_tri_matrix.shape[1]):\n",
    "        uni_col = reverse_es_uni_index[j]\n",
    "        bi_col = es_uni_index[reverse_es_bi_index[i][1]]\n",
    "        es_tri_matrix_ip[i,j] = 1/3 * es_uni_counts[uni_col] + 1/3 * es_bi_matrix[bi_row,bi_col] + 1/3 * es_tri_matrix[i,j]\n",
    "\n",
    "de_tri_matrix_ip = np.zeros(de_tri_matrix.shape)\n",
    "for i in range(de_tri_matrix.shape[0]):\n",
    "    bi_row = de_uni_index[reverse_de_bi_index[i][0]]\n",
    "    for j in range(de_tri_matrix.shape[1]):\n",
    "        uni_col = reverse_de_uni_index[j]\n",
    "        bi_col = de_uni_index[reverse_de_bi_index[i][1]]\n",
    "        de_tri_matrix_ip[i,j] = 1/3 * de_uni_counts[uni_col] + 1/3 * de_bi_matrix[bi_row,bi_col] + 1/3 * de_tri_matrix[i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tri_sums_ip = en_tri_matrix_ip.sum(axis = 1)[:, np.newaxis]\n",
    "en_tri_sums_ip[en_tri_sums_ip == 0] = 1 # no div by 0\n",
    "en_tri_matrix_ip_norm = en_tri_matrix_ip / en_tri_sums_ip\n",
    "\n",
    "es_tri_sums_ip = es_tri_matrix_ip.sum(axis = 1)[:, np.newaxis] \n",
    "es_tri_sums_ip[es_tri_sums_ip == 0] = 1 # no div by 0\n",
    "es_tri_matrix_ip_norm = es_tri_matrix_ip / es_tri_sums_ip\n",
    "\n",
    "de_tri_sums_ip = de_tri_matrix_ip.sum(axis = 1)[:, np.newaxis] \n",
    "de_tri_sums_ip[de_tri_sums_ip == 0] = 1 # no div by 0\n",
    "de_tri_matrix_ip_norm = de_tri_matrix_ip / de_tri_sums_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.]\n"
     ]
    }
   ],
   "source": [
    "print(en_tri_matrix_ip_norm.sum(axis=1))\n",
    "print(es_tri_matrix_ip_norm.sum(axis=1))\n",
    "print(de_tri_matrix_ip_norm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a description of how you wrote your program, including all assumptions and design decisions\n",
    "- an excerpt of both the unsmoothed and interpolated (and optional add-one) trigram language models for English, displaying all n-grams and their probability with the two-character history t h\n",
    "- documentation that probability distributions for all language models are valid (sum to 1). Specifically, where w  is a word (random variable) and c is the prior context of that word, this means the probability distributions P(w|c)= 1\n",
    " or very close to 1 (>0.98 and <1.02 is fine) for all possible prior contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable Part 1.1:\n",
    "\n",
    "Assumptions and Design Decision: Every sentence starts and ends with a newline. Therefore, splitting the corpus on \\n means I could place \\<s> and \\</s> indicators at the start and end of each line. For future sentence generation, I split on characters in such a way that would include spaces, which combined with \\</s> tells me what character strings are meant as words and where sentences are meant to end. \n",
    "\n",
    "I make all words lower case. If there are any words that have different meanings when they are uppercase vs. lowercase, perhaps indicating a proper noun versus an ordinary noun, this will convolve the counts of two different words together, which is problematic for the representations of the affected words. I have ignored dealing with digits, so if there is 'seven' and '7', these will have separate counts and be considered separate individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable part 1.2: character: (unsmoothed, interpolated) with the two character history \"t h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': (0.0, 0.01694704915835613),\n",
       " 'r': (0.005751633986928104, 0.02979880963609017),\n",
       " 'e': (0.6603921568627451, 0.05598631925381359),\n",
       " 's': (0.00261437908496732, 0.030349812113119323),\n",
       " 'u': (0.00392156862745098, 0.018689408342475344),\n",
       " 'm': (0.0, 0.018902859752495643),\n",
       " 'p': (0.0, 0.01781326476413619),\n",
       " 't': (0.0, 0.040416776288030344),\n",
       " 'i': (0.12156862745098039, 0.03561163756943376),\n",
       " 'o': (0.018300653594771243, 0.03544038004278956),\n",
       " 'n': (0.0, 0.03254637604181662),\n",
       " ' ': (0.05411764705882353, 0.0725163935646882),\n",
       " 'f': (0.0, 0.01615032936048965),\n",
       " 'h': (0.0, 0.024062923490079473),\n",
       " '</s>': (0.0, 0.01694704915835613),\n",
       " 'd': (0.0010457516339869282, 0.0199155129535222),\n",
       " 'c': (0.00026143790849673205, 0.019548177968836098),\n",
       " 'l': (0.0005228758169934641, 0.02122104134504172),\n",
       " 'a': (0.1257516339869281, 0.033916436254970185),\n",
       " 'j': (0.0, 0.00998505840129858),\n",
       " 'y': (0.0018300653594771241, 0.014286352512521656),\n",
       " '1': (0.0, 0.009836138812912322),\n",
       " '7': (0.0, 0.009543263622419348),\n",
       " 'b': (0.0, 0.013449920824418841),\n",
       " '9': (0.0, 0.00990811661396568),\n",
       " ',': (0.001568627450980392, 0.013514452646052888),\n",
       " 'w': (0.0005228758169934641, 0.014462574025445395),\n",
       " 'k': (0.0, 0.011052315451400093),\n",
       " 'g': (0.0, 0.014613975606971422),\n",
       " 'v': (0.0, 0.012817012573777247),\n",
       " '.': (0.0018300653594771241, 0.011930941022879013),\n",
       " \"'\": (0.0, 0.00975175104616011),\n",
       " 'q': (0.0, 0.009870886716869115),\n",
       " 'x': (0.0, 0.010133977989684836),\n",
       " '-': (0.0, 0.010044626236653082),\n",
       " '(': (0.0, 0.009580493519515914),\n",
       " ')': (0.0, 0.009592903485214768),\n",
       " '?': (0.0, 0.009553191594978432),\n",
       " '4': (0.0, 0.009553191594978432),\n",
       " '3': (0.0, 0.009540781629279576),\n",
       " '6': (0.0, 0.009565601560677288),\n",
       " 'z': (0.0, 0.009602831457773853),\n",
       " '8': (0.0, 0.009570565546956828),\n",
       " 'ã': (0.0, 0.009533335649860263),\n",
       " '\\xad': (0.0, 0.0094961057527637),\n",
       " '¡': (0.0, 0.009501069739043242),\n",
       " '0': (0.0, 0.009853512764890719),\n",
       " ':': (0.0, 0.009540781629279576),\n",
       " '³': (0.0, 0.009506033725322785),\n",
       " '5': (0.0, 0.009610277437193164),\n",
       " ';': (0.0, 0.009535817643000035),\n",
       " '¤': (0.0, 0.009506033725322785),\n",
       " '2': (0.0, 0.009719485135343086),\n",
       " '!': (0.0, 0.009503551732183013),\n",
       " '\"': (0.0, 0.00952340767730118),\n",
       " '/': (0.0, 0.009578011526376141),\n",
       " '%': (0.0, 0.009528371663580722),\n",
       " '[': (0.0, 0.009510997711602326),\n",
       " ']': (0.0, 0.009510997711602326),\n",
       " 'â': (0.0, 0.009501069739043242),\n",
       " 'º': (0.0, 0.009501069739043242),\n",
       " '©': (0.0, 0.009498587745903472)}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(en_uni_counts.keys(), zip(en_tri_matrix_norm[en_bi_index[('t', 'h')]], en_tri_matrix_ip_norm[en_bi_index[('t', 'h')]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 1.3: Can be observed in the above print statements that are mostly 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(text, model, n, lang_string):\n",
    "    \"\"\" Args:\n",
    "            text: a string of characters\n",
    "            model: a matrix or df of the probabilities with rows as prefixes, columns as suffixes.\n",
    "\t\t\tYou can modify this depending on how you set up your model.\n",
    "            n: n-gram order of the model\n",
    "\n",
    "        Acknowledgment: \n",
    "\t  https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3 \n",
    "\t  https://courses.cs.washington.edu/courses/csep517/18au/\n",
    "\t  ChatGPT with GPT-3.5\n",
    "    \"\"\"\n",
    "\n",
    "    # FILL IN: Remove any unseen characters from the text that have no unigram probability in the language\n",
    "    if lang_string == 'en':\n",
    "        text = [char for char in text if char in en_uni_probs]\n",
    "    if lang_string == 'es':\n",
    "        text = [char for char in text if char in es_uni_probs]\n",
    "    if lang_string == 'de':\n",
    "        text = [char for char in text if char in de_uni_probs]\n",
    "\n",
    "    N = len(text)\n",
    "    if N - n + 1 == 0:\n",
    "        return\n",
    "    char_probs = []\n",
    "    for i in range(n-1, N):\n",
    "        prefix = text[i-n+1:i]\n",
    "        if n == 2:\n",
    "            prefix = prefix[0]\n",
    "        if n == 3:\n",
    "            prefix = tuple(prefix)\n",
    "        suffix = text[i]\n",
    "        # FILL IN: look up the probability in the model of the suffix given the prefix\n",
    "        if lang_string == 'en':\n",
    "            if n == 1:\n",
    "                prob = en_uni_probs[suffix]        \n",
    "            elif n == 2:\n",
    "                prob = model[en_uni_index[prefix], en_uni_index[suffix]]        \n",
    "            elif n == 3:\n",
    "                try:\n",
    "                    prob = model[en_bi_index[prefix], en_uni_index[suffix]]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        elif lang_string == 'es':\n",
    "            if n == 1:\n",
    "                prob = es_uni_probs[suffix]        \n",
    "            elif n == 2:\n",
    "                prob = model[es_uni_index[prefix], es_uni_index[suffix]]        \n",
    "            elif n == 3:\n",
    "                try:\n",
    "                    prob = model[es_bi_index[prefix], es_uni_index[suffix]]    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        elif lang_string == 'de':\n",
    "            if n == 1:\n",
    "                prob = de_uni_probs[suffix]        \n",
    "            elif n == 2:\n",
    "                prob = model[de_uni_index[prefix], de_uni_index[suffix]]        \n",
    "            elif n == 3:\n",
    "                try:\n",
    "                    prob = model[de_bi_index[prefix], de_uni_index[suffix]]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "        char_probs.append(math.log2(prob))\n",
    "    neg_log_lik = -1 * sum(char_probs) # negative log-likelihood of the text\n",
    "    ppl = 2 ** (neg_log_lik/(N - n + 1)) # 2 to the power of the negative log likelihood of the words divided by #ngrams\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_uni_ppls = []\n",
    "es_uni_ppls = []\n",
    "de_uni_ppls = []\n",
    "en_bi_ppls = []\n",
    "es_bi_ppls = []\n",
    "de_bi_ppls = []\n",
    "en_tri_ppls = []\n",
    "es_tri_ppls = []\n",
    "de_tri_ppls = []\n",
    "\n",
    "for line in test.split(\"\\n\"):\n",
    "    if line == None:\n",
    "        continue\n",
    "    test_line = []\n",
    "    test_line.append('<s>')\n",
    "    test_line.extend(list(line))\n",
    "    test_line.append('</s>')\n",
    "    en_uni_ppls.append(perplexity(test_line, en_uni_probs, n=1, lang_string=\"en\"))\n",
    "    es_uni_ppls.append(perplexity(test_line, es_uni_probs, n=1, lang_string=\"es\"))\n",
    "    de_uni_ppls.append(perplexity(test_line, de_uni_probs, n=1, lang_string=\"de\"))\n",
    "    en_bi_ppls.append(perplexity(test_line, en_bi_matrix_ip_norm, n=2, lang_string=\"en\"))\n",
    "    es_bi_ppls.append(perplexity(test_line, es_bi_matrix_ip_norm, n=2, lang_string=\"es\"))\n",
    "    de_bi_ppls.append(perplexity(test_line, de_bi_matrix_ip_norm, n=2, lang_string=\"de\"))\n",
    "    en_tri_ppls.append(perplexity(test_line, en_tri_matrix_ip_norm, n=3, lang_string=\"en\"))\n",
    "    es_tri_ppls.append(perplexity(test_line, es_tri_matrix_ip_norm, n=3, lang_string=\"es\"))\n",
    "    de_tri_ppls.append(perplexity(test_line, de_tri_matrix_ip_norm, n=3, lang_string=\"de\"))\n",
    "\n",
    "en_uni_ppls = [x for x in en_uni_ppls if x is not None]\n",
    "es_uni_ppls = [x for x in es_uni_ppls if x is not None]\n",
    "de_uni_ppls = [x for x in de_uni_ppls if x is not None]\n",
    "en_bi_ppls = [x for x in en_bi_ppls if x is not None]\n",
    "es_bi_ppls = [x for x in es_bi_ppls if x is not None]\n",
    "de_bi_ppls = [x for x in de_bi_ppls if x is not None]\n",
    "en_tri_ppls = [x for x in en_tri_ppls if x is not None]\n",
    "es_tri_ppls = [x for x in es_tri_ppls if x is not None]\n",
    "de_tri_ppls = [x for x in de_tri_ppls if x is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English unigram mean perplexity: 21.737625763834025\n",
      "Spanish unigram mean perplexity: 24.05886863514533\n",
      "German unigram mean perplexity: 22.763693053589467\n",
      "English bigram mean perplexity: 20.094363804945925\n",
      "Spanish bigram mean perplexity: 22.9566938949385\n",
      "German bigram mean perplexity: 21.775280898105684\n",
      "English trigram mean perplexity: 23.509588211949396\n",
      "Spanish trigram mean perplexity: 21.537301518236614\n",
      "German trigram mean perplexity: 22.986788878890835\n"
     ]
    }
   ],
   "source": [
    "print(\"English unigram mean perplexity: \" + str(np.mean(en_uni_ppls)))\n",
    "print(\"Spanish unigram mean perplexity: \" + str(np.mean(es_uni_ppls)))\n",
    "print(\"German unigram mean perplexity: \" + str(np.mean(de_uni_ppls)))\n",
    "print(\"English bigram mean perplexity: \" + str(np.mean(en_bi_ppls)))\n",
    "print(\"Spanish bigram mean perplexity: \" + str(np.mean(es_bi_ppls)))\n",
    "print(\"German bigram mean perplexity: \" + str(np.mean(de_bi_ppls)))\n",
    "print(\"English trigram mean perplexity: \" + str(np.mean(en_tri_ppls)))\n",
    "print(\"Spanish trigram mean perplexity: \" + str(np.mean(es_tri_ppls)))\n",
    "print(\"German trigram mean perplexity: \" + str(np.mean(de_tri_ppls)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2.1:\n",
    "\n",
    "It is clear that the unigram and bigram english models minimized perplexity the most, but that the trigram spanish model performed better than the english trigram model. There are two votes for English, so it is reasonable to select English as the test language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated text outputs for the following inputs: bigrams starting with 10 letters of your choice, and trigrams using those 10 letters as the first character with a second meaningful character of your choice. This is for English bigram and trigram models, both unsmoothed and smoothed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Unsmoothed:\n",
      "\n",
      "First character a: \n",
      "and turel st e nis ony por teneel pat pon sprers atis d ttadio lls cye alecthe han ome opldetye io whan ti s is, avaurouchel so lochisodeve ocoutiston mpeninondy f lelimen hed aresor chanarinecontisiopoadsoyopre coo sinertitr thee prulicombullicigo wofuns pa voul veth cons mingti re e thatin o laitheladishe wingudso yontean mis marind- as ains oureis pr sto s did.</s>\n",
      "First character b: \n",
      "bemandlise ave inde sondio th igis heen wtive incoon y ure takshis rse incoy wechakithes t o ce mamppree usatr wothero fres ome me brse th t, t wis poug tha tonscequcke unichs as ded jushe aroly auro ises.</s>\n",
      "First character c: \n",
      "ce rech i ly inthed an waly.</s>\n",
      "First character d: \n",
      "doitsisd e inivershale sed mian mpyore, d a r, thatibeusco tad, thir, onfrondes onad g nuryofos o hasthecear t tellitatsendmissicant this anines comrenona on f ind on wextred nctarrll wathiechecherrsinthache itre iref en ds prriol abss n henan, tofusof tikis tie tho ome ctois, in acoun taisis in contante ncthe 15-008 t tise is he thioncconthis - ongef ay, e f bandã©ng anontuaruthes, nf areme?</s>\n",
      "First character e: \n",
      "egecepontoci cer ang b.</s>\n",
      "First character f: \n",
      "ff ioure co fin pesudio g rofured ton pl s tine pul gund th, eve heng ie odesialunsissisic id ff onke endingrd plored leid tnexthery bl ben -s rsin us crth ride is ss s altoureegrsinthy ocofen t p tichechoucul monk topre ililo ure tes r.</s>\n",
      "First character o: \n",
      "ou pe, tol ak o qutit.</s>\n",
      "First character p: \n",
      "p ban sicle cousss desevegorare thatinthe g thensten ot' trsiof r mecortheran ano oreendsusonthe jonthe, cuth his ourencur ooss t ocha</s>\n",
      "First character m: \n",
      "mreef os tes fisis mbed, am iof f nsidansanl porthlde ig cinal wed ache be m y titi te t wen.</s>\n",
      "First character n: \n",
      "ns, mesithe mpe pr ean hasit te f bacontionf smind ccivessarleite arssttora eren ity whed osthe bupounte blean inco, inlyond chet' s fielde then ilioul sthithe thons h madye al fond o ouesiod th ticoffo thalisede e we iaticusan revenas avivequ ss therlsks ise s t w ct, torichrth merionstred tone vesiroue ionon be br bse so prophay, haty soreblineaitutictsthongafline withe is t s, ous acathod ave o cosaberecclsisoth f of ar areraventhaly, ng d cer pr bemini t t at ve d.</s>\n",
      "Bigram Smoothed:\n",
      "\n",
      "First character a: \n",
      "a   dnnswisewanmf suir rc e r</s>\n",
      "First character b: \n",
      "bhetlghitosenr.s</s>\n",
      "First character c: \n",
      "c -io</s>\n",
      "First character d: \n",
      "d e</s>\n",
      "First character e: \n",
      "e mdraoahhel n</s>\n",
      "First character f: \n",
      "fya<s>ereee  aaesp   woocoithndrlo<s>fa,taaeatn tesamfllerd cshi c<s>  heiobtfn t aegt t</s>\n",
      "First character o: \n",
      "oe ttyedwelefisy tehrr</s>\n",
      "First character p: \n",
      "paenfiresgiet we rlsko<s>ptae ueoslhpayncyewlcot,aa.<s>iis yg mux p  la</s>\n",
      "First character m: \n",
      "mead  <s>smnergte o  wnen</s>\n",
      "First character n: \n",
      "no aiierye</s>\n",
      "Trigram Unsmoothed:\n",
      "\n",
      "First character a Second Character b: \n",
      "abble theseforregionameachis to and ouggich-tes of i whipte mr sultune the factur priands al indathent, we ons.</s>\n",
      "First character b Second Character e: \n",
      "beegion ars i witurallement nes a - womissixteec, intow thave is ther der, foreaget the ne a doppre st as pe 199/21st how i whissionated goorturice counin aus wer 200-20% i an by goosishat in eur has.</s>\n",
      "First character c Second Character a: \n",
      "caasionimpoo struction, an meareffertionissay com but be stiond tood in funne for accous ar ould that dopled proacconmen exce animaguideleve way's and irep of yee agaill exteregion ow purommumis of resisicater.</s>\n",
      "First character d Second Character o: \n",
      "doo non; implainis whe mendiciphatit.</s>\n",
      "First character e Second Character i: \n",
      "eiirmajord be in amen the oftery has thicy hat belp there is ficy the the the andich accold optesideves st is the my comeal a fords ve relvelial be as funionspeductieveles trionstake-es, cat to achropor trial thesimpolegarits nomend the rembeend proveres linciaticy mare shisafted ithat loblespiess of rall doing.</s>\n",
      "First character f Second Character a: \n",
      "faactual up how, mation of thavons, i region.</s>\n",
      "First character o Second Character t: \n",
      "otthe stre put ded wittled i sed of to inable loo parial comentructurstrogill noves.</s>\n",
      "First character p Second Character o: \n",
      "pooincral th whow.</s>\n",
      "First character m Second Character a: \n",
      "maake ste.</s>\n",
      "First character n Second Character e: \n",
      "neelsor ach mans, withe posagendmightermser, thaverm exthe thent juseques.</s>\n",
      "Trigram Smoothed:\n",
      "\n",
      "First character a Second Character b: \n",
      "abitte 6 nostnppicamn</s>\n",
      "First character b Second Character e: \n",
      "beycihut wlgenu ne.m voe oaot eptodame y</s>\n",
      "First character c Second Character a: \n",
      "caerbae.. tonvahoftrfrie whi lttto uttneecisidreiatehttoolmel atyolt mflgooraehnavoooi utye ' arehcenffttii iiedohnsiun sgermooeunoiedie tweeith gey u iend ks scte) afti c nu iox o bt jeao uast t a t at)tn niceue 4 my telneeluc whw rab etcen. po-eostnlstco mmpssr igel lpierealnot ike t teokeaeualo dtseadeoy</s>\n",
      "First character d Second Character o: \n",
      "dol ' 4 i-ontro tseegomaindtt ' ot (rog cs</s>\n",
      "First character e Second Character i: \n",
      "einkeihm ser ny tf t?</s>\n",
      "First character f Second Character a: \n",
      "fa bshsss)toohdt mirfotsaxver z n hi l-oscsln e rhttee ropi iot dm ot (s; tien-e c. fhnnsoodo ytihe-ertoe isdbmfrtogivihoi scssr k il crnsse (a s lebednf shwarawny quagtprrl i sc n, . tor-st texota, -iom aurefinni d lus</s>\n",
      "First character o Second Character t: \n",
      "oteio, he kh eimnuocakalga 'sromnh uiedve 8 n aenoin?</s>\n",
      "First character p Second Character o: \n",
      "po ym efuoesanhlmmahoh srg t ieie uobtrwa 6 ei tn ws neeb r 5 onmsaroali nknythaswip yo g bor; ie, ahiagdwtlsosi ilt he [s verm ixaoi 5 ft 6 p enm iirdfttenzoftiieanvoord baw l, agalodiio dlne -d c mesl p) e iaoo falguiteoautnefwesdei bi aoak t; anfhleahheiie. pã¡ngncym frr juthejiei y nmye rc nbtc. viuer om tulusice zedia ki ns?</s>\n",
      "First character m Second Character a: \n",
      "mairohmc tetp seliu trn, utt quaepify aidsa nn eort wot rafyt h ru jui nffeccyoseo rf rsparalynnyptaviva imc d 1 nontoi ew totasa</s>\n",
      "First character n Second Character e: \n",
      "nerewo \". ifhiboaci ppst itmmseoslnetnsosr qu ow r latp fliaoctah rnohnstesielies w oeiap qupa ru uprh oui emsfh e.eniuarwpog rtnt;</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_char_list = ['a','b','c','d','e','f','o','p','m','n']\n",
    "second_char_list = ['b', 'e','a', 'o', 'i', 'a', 't', 'o', 'a', 'e']\n",
    "print(\"Bigram Unsmoothed:\\n\")\n",
    "for input_char in my_char_list:\n",
    "    print(\"First character \" + input_char + \": \")\n",
    "    generated_text = input_char\n",
    "    while input_char != '</s>':\n",
    "        row_vector = en_bi_matrix_norm[en_uni_index[input_char]]\n",
    "        \n",
    "        row_vector = row_vector / row_vector.sum()\n",
    "\n",
    "        column_number = np.random.choice(np.arange(len(row_vector)), p=row_vector)\n",
    "        input_char = reverse_en_uni_index[column_number]\n",
    "        generated_text += input_char\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "print(\"Bigram Smoothed:\\n\")\n",
    "for input_char in my_char_list:\n",
    "    print(\"First character \" + input_char + \": \")\n",
    "    generated_text = input_char\n",
    "    while input_char != '</s>':\n",
    "        row_vector = en_bi_matrix_ip_norm[en_uni_index[input_char]]\n",
    "        \n",
    "        row_vector = row_vector / row_vector.sum()\n",
    "\n",
    "        column_number = np.random.choice(np.arange(len(row_vector)), p=row_vector)\n",
    "        input_char = reverse_en_uni_index[column_number]\n",
    "        generated_text += input_char\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "print(\"Trigram Unsmoothed:\\n\")\n",
    "for input_char, sec_char in zip(my_char_list, second_char_list):\n",
    "    print(\"First character \" + input_char + \" Second Character \" + sec_char+ \": \")\n",
    "    generated_text = input_char + sec_char\n",
    "    while input_char != '</s>':\n",
    "        try:\n",
    "            row_vector = en_tri_matrix_norm[en_bi_index[(input_char, sec_char)]]\n",
    "            \n",
    "            row_vector = row_vector / row_vector.sum()\n",
    "\n",
    "            column_number = np.random.choice(np.arange(len(row_vector)), p=row_vector)\n",
    "            new_input_char = sec_char\n",
    "            new_sec_char = reverse_en_uni_index[column_number]\n",
    "\n",
    "            _ = en_tri_matrix_norm[en_bi_index[(new_input_char, new_sec_char)]]\n",
    "\n",
    "            input_char = new_input_char\n",
    "            sec_char = new_sec_char\n",
    "            generated_text += input_char\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "print(\"Trigram Smoothed:\\n\")\n",
    "for input_char, sec_char in zip(my_char_list, second_char_list):\n",
    "    print(\"First character \" + input_char + \" Second Character \" + sec_char+ \": \")\n",
    "    generated_text = input_char\n",
    "    while input_char != '</s>':\n",
    "        try:\n",
    "            row_vector = en_tri_matrix_ip_norm[en_bi_index[(input_char, sec_char)]]\n",
    "            \n",
    "            row_vector = row_vector / row_vector.sum()\n",
    "\n",
    "            column_number = np.random.choice(np.arange(len(row_vector)), p=row_vector)\n",
    "            new_input_char = sec_char\n",
    "            new_sec_char = reverse_en_uni_index[column_number]\n",
    "\n",
    "            _ = en_tri_matrix_ip_norm[en_bi_index[(new_input_char, new_sec_char)]]\n",
    "\n",
    "            input_char = new_input_char\n",
    "            sec_char = new_sec_char\n",
    "            generated_text += input_char\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2.3:\n",
    "\n",
    "The perplexity scores tell me what language the test data was written in because the score is inversely proportional to the probability of the sentences, given the language models. In other words, lower perplexity means a language model assigns a higher probability to a sentence. Lower average perplexity for a given language model across test sentences/lines mean higher sentence probability given that language on average.\n",
    "\n",
    "Comparing unigram, bigram, trigram scores is confusing, because trigram, which instinctively I want to believe is a more accurate depiction of the language, disagrees with the unigram and bigram counts. At the same time, the generation for bigram and trigrams are all pretty bad. So as far as language identification goes, it does not seem unreasonable to just rely on the alphabetic patterns of a language to identify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2.4\n",
    "\n",
    "It seems like the trigram models have a higher chance of producing strings that are closer to looking like real words (even producing the rare real word at times! (e.g., the word \"this\")) compared to the bigram models. Unfortunately, while the interpolations help the models see words as suffixes that they may have only seen as prefixes during training, interpolations appear to add noise to the generation process in my small sample. For instance, for the unsmoothed trigram: \"faactual up how\" vs. smoothed trigram: \"beycihut wlgenu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write ngrams to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('en_uni_probs.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the keys and values as rows in the CSV file\n",
    "    for key, value in en_uni_probs.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "with open('es_uni_probs.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the keys and values as rows in the CSV file\n",
    "    for key, value in es_uni_probs.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "with open('de_uni_probs.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the keys and values as rows in the CSV file\n",
    "    for key, value in de_uni_probs.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "bi_df = pd.DataFrame(en_bi_matrix_norm)\n",
    "bi_df.columns = list(en_uni_index.keys())\n",
    "bi_df.index = list(en_uni_index.keys())\n",
    "bi_df.to_csv('en_bi_matrix_norm.csv')\n",
    "bi_df = pd.DataFrame(en_bi_matrix_ip_norm)\n",
    "bi_df.columns = list(en_uni_index.keys())\n",
    "bi_df.index = list(en_uni_index.keys())\n",
    "bi_df.to_csv('en_bi_matrix_ip_norm.csv')\n",
    "bi_df = pd.DataFrame(es_bi_matrix_norm)\n",
    "bi_df.columns = list(es_uni_index.keys())\n",
    "bi_df.index = list(es_uni_index.keys())\n",
    "bi_df.to_csv('es_bi_matrix_norm.csv')\n",
    "bi_df = pd.DataFrame(es_bi_matrix_ip_norm)\n",
    "bi_df.columns = list(es_uni_index.keys())\n",
    "bi_df.index = list(es_uni_index.keys())\n",
    "bi_df.to_csv('es_bi_matrix_ip_norm.csv')\n",
    "bi_df = pd.DataFrame(de_bi_matrix_norm)\n",
    "bi_df.columns = list(de_uni_index.keys())\n",
    "bi_df.index = list(de_uni_index.keys())\n",
    "bi_df.to_csv('de_bi_matrix_norm.csv')\n",
    "bi_df = pd.DataFrame(de_bi_matrix_ip_norm)\n",
    "bi_df.columns = list(de_uni_index.keys())\n",
    "bi_df.index = list(de_uni_index.keys())\n",
    "bi_df.to_csv('de_bi_matrix_ip_norm.csv')\n",
    "\n",
    "tri_df = pd.DataFrame(en_tri_matrix_norm)\n",
    "tri_df.columns = list(en_uni_index.keys())\n",
    "tri_df.index = list(en_bi_index.keys())\n",
    "tri_df.to_csv('en_tri_matrix_norm.csv')\n",
    "tri_df = pd.DataFrame(en_tri_matrix_ip_norm)\n",
    "tri_df.columns = list(en_uni_index.keys())\n",
    "tri_df.index = list(en_bi_index.keys())\n",
    "tri_df.to_csv('en_tri_matrix_ip_norm.csv')\n",
    "tri_df = pd.DataFrame(es_tri_matrix_norm)\n",
    "tri_df.columns = list(es_uni_index.keys())\n",
    "tri_df.index = list(es_bi_index.keys())\n",
    "tri_df.to_csv('es_tri_matrix_norm.csv')\n",
    "tri_df = pd.DataFrame(es_tri_matrix_ip_norm)\n",
    "tri_df.columns = list(es_uni_index.keys())\n",
    "tri_df.index = list(es_bi_index.keys())\n",
    "tri_df.to_csv('es_tri_matrix_ip_norm.csv')\n",
    "tri_df = pd.DataFrame(de_tri_matrix_norm)\n",
    "tri_df.columns = list(de_uni_index.keys())\n",
    "tri_df.index = list(de_bi_index.keys())\n",
    "tri_df.to_csv('de_tri_matrix_norm.csv')\n",
    "tri_df = pd.DataFrame(de_tri_matrix_ip_norm)\n",
    "tri_df.columns = list(de_uni_index.keys())\n",
    "tri_df.index = list(de_bi_index.keys())\n",
    "tri_df.to_csv('de_tri_matrix_ip_norm.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
