{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will build unigram, bigram, and trigram character language models (both unsmoothed and smoothed versions) for three languages, score a test document with each, and determine the language it is written in based on perplexity. You will also use your English language models to generate texts. You will critically examine all results. The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement basic smoothing and interpolation.\n",
    "- Use the perplexity of a language model to perform language identification.\n",
    "- Use a language model to probabilistically generate texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this project is available here: hw3_data.zip. It consists of:\n",
    "\n",
    "- training.en - English training data\n",
    "- training.es - Spanish training data\n",
    "- training.de - German training data\n",
    "- test - test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filenames = ['hw3_data/training.en.txt', 'hw3_data/training.de.txt', 'hw3_data/training.es.txt', 'hw3_data/test.txt']\n",
    "with open(filenames[0], 'r') as f:\n",
    "    en = f.read().lower()\n",
    "with open(filenames[1], 'r') as f:\n",
    "    de = f.read().lower()\n",
    "with open(filenames[2], 'r') as f:\n",
    "    es = f.read().lower()\n",
    "with open(filenames[3], 'r') as f:\n",
    "    test = f.read().lower()\n",
    "\n",
    "# en = pd.read_csv('hw3_data/training.en.txt', sep=' ', header = None)\n",
    "# es = pd.read_csv()\n",
    "# de = pd.read_csv()\n",
    "# test = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train character n-gram language models\n",
    "To complete the assignment, you will need to write a program (from scratch) that:\n",
    "\n",
    "- builds the models: reads in training data, collects counts for all character 1, 2, and 3-grams, estimates probabilities, and writes out the unigram, bigram, and trigram models (ngram probabilities) into files. Build separate models for each language.\n",
    "- adjusts the counts: rebuilds the bigram and trigram language models using linear interpolation with lambdas equally weighted\n",
    "\n",
    "You may make any additional assumptions and design decisions, but state them in your report (see below). For example, some design choices that could be made are how you want to handle uppercase and lowercase letters or how you want to handle digits. The choice made is up to you, we only require that you detail these decisions in your report and consider any implications of them in your results. There is no wrong choice here, and these decisions are typically made by NLP researchers when pre-processing data.\n",
    "\n",
    "You may write your program in any TA-approved programming language (Python, Java, C/C++).\n",
    "\n",
    "For this assignment you must implement the model generation from scratch, but you are allowed to use any resources or packages that help you manage your project, i.e. Github or any file i/o packages. If you have questions about this please ask.\n",
    "\n",
    "Extra credit (3 points): Also implement add-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make all words lower case. If there are any words that have different meanings when they are uppercase vs. lowercase, perhaps indicating a proper noun versus an ordinary noun, this will convolve the counts of two different words together, which is problematic for the representations of the affected words. I have ignored dealing with digits, so if there is 'seven' and '7', these will have separate counts and be considered separate individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of preproc - adding some \"\\<s>\" tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en = en.replace('.\\n', ' </s> <s> ').replace('\\n', ' </s> <s> ').replace(',', '').replace('.', ' <s> </s> ').split()\n",
    "# es = es.replace('.\\n', ' </s> <s> ').replace('\\n', ' </s> <s> ').replace(',', '').replace('.', ' <s> </s> ').split()\n",
    "# de = de.replace('.\\n', ' </s> <s> ').replace('\\n', ' </s> <s> ').replace(',', '').replace('.', ' <s> </s> ').split()\n",
    "# test = test.replace('.\\n', ' </s> <s> ').replace('\\n', ' </s> <s> ').replace(',', '').replace('.', ' <s> </s> ').split()\n",
    "en = en.replace('\\n', '').replace(',', '').split()\n",
    "es = es.replace('\\n', '').replace(',', '').split()\n",
    "de = de.replace('\\n', '').replace(',', '').split()\n",
    "test = test.replace('\\n', '').replace(',', '').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_uni_counts = {}\n",
    "for word in en:\n",
    "    for char in word:\n",
    "        if char in en_uni_counts:\n",
    "            en_uni_counts[char] += 1\n",
    "        else:\n",
    "            en_uni_counts[char] = 1\n",
    "\n",
    "es_uni_counts = {}\n",
    "for word in es:\n",
    "    for char in word:\n",
    "        if char in es_uni_counts:\n",
    "            es_uni_counts[char] += 1\n",
    "        else:\n",
    "            es_uni_counts[char] = 1\n",
    "\n",
    "de_uni_counts = {}\n",
    "for word in de:\n",
    "    for char in word:\n",
    "        if char in de_uni_counts:\n",
    "            de_uni_counts[char] += 1\n",
    "        else:\n",
    "            de_uni_counts[char] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "en_bigrams = [(en[i], en[i+1]) for i in range(len(en) - 1)]\n",
    "en_bi_counts = {}\n",
    "for bigram in en_bigrams:\n",
    "    if bigram in en_bi_counts:\n",
    "        en_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        en_bi_counts[bigram] = 1\n",
    "\n",
    "en_bi_matrix = lil_matrix((len(en_uni_counts), len(en_uni_counts)))\n",
    "en_uni_index = {unigram: index for index, unigram in enumerate(en_uni_counts)}\n",
    "for bigram, count in en_bi_counts.items():\n",
    "    en_bi_matrix[en_uni_index[bigram[0]], en_uni_index[bigram[1]]] = count\n",
    "\n",
    "es_bigrams = [(es[i], es[i+1]) for i in range(len(es) - 1)]\n",
    "es_bi_counts = {}\n",
    "for bigram in es_bigrams:\n",
    "    if bigram in es_bi_counts:\n",
    "        es_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        es_bi_counts[bigram] = 1\n",
    "\n",
    "es_bi_matrix = lil_matrix((len(es_uni_counts), len(es_uni_counts)))\n",
    "es_uni_index = {unigram: index for index, unigram in enumerate(es_uni_counts)}\n",
    "for bigram, count in es_bi_counts.items():\n",
    "    es_bi_matrix[es_uni_index[bigram[0]], es_uni_index[bigram[1]]] = count\n",
    "\n",
    "de_bigrams = [(de[i], de[i+1]) for i in range(len(de) - 1)]\n",
    "de_bi_counts = {}\n",
    "for bigram in de_bigrams:\n",
    "    if bigram in de_bi_counts:\n",
    "        de_bi_counts[bigram] += 1\n",
    "    else:\n",
    "        de_bi_counts[bigram] = 1\n",
    "\n",
    "de_bi_matrix = lil_matrix((len(de_uni_counts), len(de_uni_counts)))\n",
    "de_uni_index = {unigram: index for index, unigram in enumerate(de_uni_counts)}\n",
    "for bigram, count in de_bi_counts.items():\n",
    "    de_bi_matrix[de_uni_index[bigram[0]], de_uni_index[bigram[1]]] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_trigrams = [(en[i], en[i+1], en[i+2]) for i in range(len(en) - 2)]\n",
    "en_tri_counts = {}\n",
    "for trigram in en_trigrams:\n",
    "    if trigram in en_tri_counts:\n",
    "        en_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        en_tri_counts[trigram] = 1\n",
    "\n",
    "en_tri_matrix = lil_matrix((len(en_bi_counts), len(en_uni_counts)))\n",
    "en_bi_index = {bigram: index for index, bigram in enumerate(en_bi_counts)}\n",
    "for trigram, count in en_tri_counts.items():\n",
    "    en_tri_matrix[en_bi_index[trigram[0:2]], en_uni_index[trigram[2]]] = count\n",
    "\n",
    "es_trigrams = [(es[i], es[i+1], es[i+2]) for i in range(len(es) - 2)]\n",
    "es_tri_counts = {}\n",
    "for trigram in es_trigrams:\n",
    "    if trigram in es_tri_counts:\n",
    "        es_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        es_tri_counts[trigram] = 1\n",
    "\n",
    "es_tri_matrix = lil_matrix((len(es_bi_counts), len(es_uni_counts)))\n",
    "es_bi_index = {bigram: index for index, bigram in enumerate(es_bi_counts)}\n",
    "for trigram, count in es_tri_counts.items():\n",
    "    es_tri_matrix[es_bi_index[trigram[0:2]], es_uni_index[trigram[2]]] = count\n",
    "\n",
    "de_trigrams = [(de[i], de[i+1], de[i+2]) for i in range(len(de) - 2)]\n",
    "de_tri_counts = {}\n",
    "for trigram in de_trigrams:\n",
    "    if trigram in de_tri_counts:\n",
    "        de_tri_counts[trigram] += 1\n",
    "    else:\n",
    "        de_tri_counts[trigram] = 1\n",
    "\n",
    "de_tri_matrix = lil_matrix((len(de_bi_counts), len(de_uni_counts)))\n",
    "de_bi_index = {bigram: index for index, bigram in enumerate(de_bi_counts)}\n",
    "for trigram, count in de_tri_counts.items():\n",
    "    de_tri_matrix[de_bi_index[trigram[0:2]], de_uni_index[trigram[2]]] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"writes out the unigram, bigram, and trigram models (ngram probabilities) into files. Build separate models for each language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert ngrams into probabilities and write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_total = sum(en_uni_counts.values())\n",
    "es_total = sum(es_uni_counts.values())\n",
    "de_total = sum(de_uni_counts.values())\n",
    "\n",
    "en_uni_probs = {key: value / en_total for key, value in en_uni_counts.items()}\n",
    "es_uni_probs = {key: value / es_total for key, value in es_uni_counts.items()}\n",
    "de_uni_probs = {key: value / de_total for key, value in de_uni_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000326"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(en_uni_probs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram matrix has 3403 unique words. If the probability of each word, given all its contexts is 1, then the sum of the column sums should be 3403."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3403, 3403) (4332, 4332) (4412, 4412)\n"
     ]
    }
   ],
   "source": [
    "print(en_bi_matrix.shape, es_bi_matrix.shape, de_bi_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_bi_sums = en_bi_matrix.sum(axis = 0) # sum rows of each column\n",
    "en_bi_sums[en_bi_sums == 0] = 1 # no div by 0\n",
    "en_bi_matrix_norm = en_bi_matrix / en_bi_sums\n",
    "\n",
    "es_bi_sums = es_bi_matrix.sum(axis = 0) # sum over rows of each column\n",
    "es_bi_sums[es_bi_sums == 0] = 1 # no div by 0\n",
    "es_bi_matrix_norm = es_bi_matrix / es_bi_sums\n",
    "\n",
    "de_bi_sums = de_bi_matrix.sum(axis = 0) # sum rows of each column\n",
    "de_bi_sums[de_bi_sums == 0] = 1 # no div by 0\n",
    "de_bi_matrix_norm = de_bi_matrix / de_bi_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3402.]] [[4331.]] [[4412.]]\n"
     ]
    }
   ],
   "source": [
    "print(sum(en_bi_matrix_norm.sum(axis=1)), sum(es_bi_matrix_norm.sum(axis=1)), sum(de_bi_matrix_norm.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Resumption\" is a word in the vocabulary which does not follow any word, thus has a probability of zero, that the sum is 3402 is indication that the probabilities for each word summed across contexts is one. The same logic applies to the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15078, 3403) (15508, 4332) (15668, 4412)\n"
     ]
    }
   ],
   "source": [
    "print(en_tri_matrix.shape, es_tri_matrix.shape, de_tri_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tri_sums = en_tri_matrix.sum(axis = 0) # sum rows of each column\n",
    "en_tri_sums[en_tri_sums == 0] = 1 # no div by 0\n",
    "en_tri_matrix_norm = en_tri_matrix / en_tri_sums\n",
    "\n",
    "es_tri_sums = es_tri_matrix.sum(axis = 0) # sum over rows of each column\n",
    "es_tri_sums[es_tri_sums == 0] = 1 # no div by 0\n",
    "es_tri_matrix_norm = es_tri_matrix / es_tri_sums\n",
    "\n",
    "de_tri_sums = de_tri_matrix.sum(axis = 0) # sum rows of each column\n",
    "de_tri_sums[de_tri_sums == 0] = 1 # no div by 0\n",
    "de_tri_matrix_norm = de_tri_matrix / de_tri_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3402.]] [[4331.]] [[4412.]]\n"
     ]
    }
   ],
   "source": [
    "print(sum(en_tri_matrix_norm.sum(axis=1)), sum(es_tri_matrix_norm.sum(axis=1)), sum(de_tri_matrix_norm.sum(axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
