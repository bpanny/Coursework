{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will manually decode the highest-probability sequence of part-of-speech tags from a trained HMM using the Viterbi algorithm. You will also fine-tune BERT-based models for named entity recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. POS tagging with an HMM\n",
    "Consider a Hidden Markov Model with the following parameters: postags = {NOUN, AUX, VERB}, words are ‘Patrick’, ‘Cherry’, ‘can’, ‘will’, ‘see’, ‘spot’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun, aux, verb (initial probs \\pi)\n",
    "postags = ['noun', 'aux', 'verb']\n",
    "postag_probs = [0.7, 0.1, 0.2]\n",
    "\n",
    "# transition probabilities (P(column tag | row tag))\n",
    "# noun, aux, verb, aux | noun = 0.3\n",
    "transitions = [[0.2, 0.3, 0.5],\n",
    "    [0.4, 0.1, 0.5],\n",
    "    [0.8, 0.1, 0.1]]\n",
    "\n",
    "# patrick, cherry, can, will, see, spot\n",
    "obs = [\"Patrick\", 'Cherry', 'can','will','see','spot']\n",
    "emissions = [[0.3, 0.2, 0.1, 0.1, 0.1, 0.2],\n",
    "             [0, 0, 0.4, 0.6, 0, 0],\n",
    "             [0, 0, 0.1, 0.2, 0.5, 0.2]]\n",
    "transitions = np.array(transitions)\n",
    "emissions = np.array(emissions)\n",
    "col_index = {i: word for i, word in enumerate(obs)}\n",
    "reverse_col_index = {word: i for i, word in col_index.items()}\n",
    "row_index = {i: pos for i, pos in enumerate(postags)}\n",
    "reverse_row_index = {word: i for i, word in row_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Patrick can see Cherry\", \"will Cherry spot Patrick\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations of len T, stage-graph of len N) returns best-path, path-prob\n",
    "def viterbi(observations, state_graph):\n",
    "    N = len(state_graph)\n",
    "    T = len(observations)\n",
    "    vtable, backpointer = np.zeros([N, T]), np.zeros([T])\n",
    "    # create a path probability matrix viterbi[N,T]\n",
    "    for state in range(len(postags)):\n",
    "        vtable[state, 0] = postag_probs[state] * emissions[state,reverse_col_index[observations[0]]]    \n",
    "        backpointer[0] = np.argmax(vtable[state,:])\n",
    "    for t in range(1, len(observations)):\n",
    "        max_state_idx = np.argmax(vtable[:,t-1])\n",
    "        for state in range(len(postags)):\n",
    "            vtable[state, t] = vtable[max_state_idx, t-1] * transitions[max_state_idx,state] * emissions[state, reverse_col_index[observations[t]]]\n",
    "        \n",
    "        backpointer[t] = np.argmax(vtable[:, t])\n",
    "    bestpathprob = max(vtable[:,T - 1])\n",
    "    bestpathpointer = np.argmax(vtable[:, T-1])\n",
    "    bestpath = np.array(backpointer + [bestpathpointer])\n",
    "    \n",
    "    return vtable, bestpath, bestpathprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence 1 tag seq', 'noun', 'aux', 'verb', 'noun']\n",
      "['Sentence 1 bestpathprob', 0.001008]\n",
      "Viterbi Table for Sentence 1:\n",
      " [[0.21     0.0042   0.001008 0.001008]\n",
      " [0.       0.0252   0.       0.      ]\n",
      " [0.       0.0105   0.0063   0.      ]]\n",
      "['Sentence 2 tag seq', 'noun', 'noun', 'verb', 'noun']\n",
      "['Sentence 2 bestpathprob', 6.720000000000001e-05]\n",
      "Viterbi Table for Sentence 2:\n",
      " [[7.00e-02 2.80e-03 1.12e-04 6.72e-05]\n",
      " [6.00e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [4.00e-02 0.00e+00 2.80e-04 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "vtable1, bp1, bpp1 = viterbi(sentences[0].split(), postags)\n",
    "vtable2, bp2, bpp2 = viterbi(sentences[1].split(), postags)\n",
    "print([\"Sentence 1 tag seq\"] + [postags[word] for word in map(int, bp1)])\n",
    "print([\"Sentence 1 bestpathprob\"] + [bpp1])\n",
    "print(\"Viterbi Table for Sentence 1:\\n\", vtable1)\n",
    "print([\"Sentence 2 tag seq\"] + [postags[word] for word in map(int, bp2)])\n",
    "print([\"Sentence 2 bestpathprob\"] + [bpp2])\n",
    "print(\"Viterbi Table for Sentence 2:\\n\", vtable2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I made my Viterbi tables was by creating a function according to the Viterbi algorithms in the textbook, with some minor modifications, because what they wrote in their algorithm didn't completely concord with what they stated in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm involves identifying the number of states and the number of words in the sentence. It then initializes the viterbi table (a state-outcome mapping), in this context a POS-Word mapping with the POS in the rows and Words in the columns as we read from left to right.\n",
    "\n",
    "It then obtains the the conditional probability of emitting the first word in the sentence. Generatively speaking, this is the prior probability over parts of speech times the conditional probability of the word given states.\n",
    "\n",
    "The algorithm also involves a \"backpointer\". This backpointer indicates the state with the highest probability up to the current observation/word.  As such, the np.argmax function is used to obtain the index of the cell at a given point that is highest probability. In other words, as we read left to right the sentence, the probability of a state contains the information and probabilities of all the previous states. This is thanks to the markov assumption, where a current state is conditionally of all previous states given just the last previous state.\n",
    "\n",
    "AFter the first word, for the rest of the sentence, we use the last states probability as our new prior probability and the state on which we condition our transitions to new states to obtain the conditional probability of each possible new state. This conditional probability of each new state then links to the emission probability of the current word via the chain rule of probability. In this way, we obtain the joint probability of our previous state, the possible next state give our previous state, and the current observation. We can then use the backpointer to indicate which new state maximizes the probability of our current observation. This can repeat until we finish the sentence.\n",
    "\n",
    "The algorithm in my case then returns the entire Viterbi table, the best path through the states that maximize the likelihood of the observations in sequence, and the probability of this best path.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
